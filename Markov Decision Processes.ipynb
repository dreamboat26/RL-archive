{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to construct the environment for the Casinos scenario presented on [this article](https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-markov-decision-processes-7d8469a8a782). Here, we're going to mainly cover the implementation details. For a deeper look onto how MDP's work and how they are conceptualized, it's recommended to read the whole article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Casinos Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Casinos environment is a simple scenario I presented as a middle point between complex MDPs and the already familiar Multi-Armed Bandit Scenario. It presents two states, which represent two distinct casinos. Each of those independent states behaves very much like the Multi-Armed Bandit Scenario. They have some number of Armed Bandits they can pull, and those actions will not change the agent's current state. To allow transition between the two casinos, an additional action was added to both states, called **Take a bus**. This action is non-deterministic, which means that with some certain probability, the environment might respond differently to what's expected. Here's a full diagram of the MDP.\n",
    "\n",
    "<img src='assets/casinos_mdp.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We're going to be using the gym package to construct our custom environment. We will be going step-by-step, and each step will add some detail to the environment.\n",
    "\n",
    "### Gym Custom Environment Template\n",
    "Below, you will find the backbone of a gym environment. This has all the necessary functions to create an acceptable environment. We will start adding details to this template in order to construct our desired environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CasinosEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Casinos custom environment, built with the gym interface\n",
    "    \"\"\"\n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CasinosmEnv, self).__init__()\n",
    "        \n",
    "        # Here, we declare our initial configuration\n",
    "        # Usually, this includes defining the action\n",
    "        # and observation step\n",
    "        pass\n",
    "        \n",
    "    def step(self, action):\n",
    "        # This function executes a one-step simulation\n",
    "        # in our environment. Receives the desired action\n",
    "        # the agent has taken as input\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        # Restarts the environment to a starting position.\n",
    "        # Usually, changes in the environment will be undone,\n",
    "        # and the environment is reseeded\n",
    "        pass\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        # Display the environment in some way. The mode defines\n",
    "        # in which way we desire to display it.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go through each individual function or method. Before we do so, there are some special remarks to do about the template:\n",
    "\n",
    "- It inherits from the `gym.Env` class\n",
    "- Our class contains a global property called `metadata`. This parameter usually contains information meant for the developer. Most of the time, it includes a list of all possible ways the developer is able to render the specific environment. In this case, only `'human'` is allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "The initialization is where we add any primary configurations required for the environment to work properly. Most of the time, we only need to define two required properties of the environment: the `action_space` and `observation_space`. Both of these values define the number of actions the agent may find, as well as what it should expect to receive from the state of the environment. As a sidenote, we use the name \"observation\" here because sometimes, the whole state of the environment isn't accessible to the agent. In our case, observation and state will be the same thing, which means that an agent interacting with the environment will have full access to the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasinosEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Casinos custom environment, built with the gym interface\n",
    "    \"\"\"\n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        \n",
    "        # Here, we declare our initial configuration\n",
    "        # Usually, this includes defining the action\n",
    "        # and observation step\n",
    "        \n",
    "        self.observation_space = spaces.Discrete(2)\n",
    "        self.action_space = spaces.Tuple((spaces.Discrete(3), spaces.Discrete(4)))\n",
    "        self.agent_pos = np.random.randint(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our initialization method we have declared both necessary properties. There are many options offered by gym to define how our observation and action space looks like. Most of the time, we can find out what option to choose from the problem at hand. \n",
    "\n",
    "Our environment contains two distinct states (Casino 1, Casino 2). They are discrete, and so we use the `spaces.Discrete(2)` option.\n",
    "for our actions, the MDP described previously had a different set of actions according to which casino we find ourselves in. In each casino, we also have a discrete number of actions we can take. For the first casino we only have three actions, while for the second we have 4. For this reason, we used a `spaces.Tuple`, which contains two sets of discrete spaces, one for each state.\n",
    "\n",
    "Lastly, we added an additional variable, which stores the agent's current position. This allows the environment to keep track of the agent's state, and it is also what the agent will see from any observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step\n",
    "Our step function contains most of the interaction dynamics of the environment. This is the function our agent calls everytime it wants to take an action upon the environment. Because of this, the step function is in charge of evaluation such action, and transitioning to the corresponding state. It also defines how much reward the agent should receive, the next observation that the agent will use to take future actions, and whether the agent reached an ending state.\n",
    "\n",
    "We have all the necessary information with the diagram above, so we will turn it into code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasinosEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Casinos custom environment, built with the gym interface\n",
    "    \"\"\"\n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        return {'state': self.agent_pos}\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Our actions will be defined as such:\n",
    "        - Casino 1 (state 0):\n",
    "            - Action 0: Pull Arm 0, go to state 0, get reward 0.5\n",
    "            - Action 1: Pull Arm 1, go to state 0, get reward -1\n",
    "            - Action 2: Take a bus,\n",
    "                70% of times: go to state 1, get reward -2\n",
    "                30% of times: go to state 0, get reward -0.5\n",
    "                \n",
    "        - Casino 2 (state 1):\n",
    "            - Action 0: Pull Arm 0, go to state 1, get reward -2\n",
    "            - Action 1: Pull Arm 1, go to state 1, get reward 1.2\n",
    "            - Action 2: Pull Arm 2, go to state 1, get reward -1.5\n",
    "            - Action 3: Take a bus,\n",
    "                80% of times: go to state 0, get reward -2\n",
    "                30% of times: go to state 1, get reward -0.5\n",
    "                \n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        \n",
    "        # dictionary of functions that return [reward, next_state] transitions for each action\n",
    "        state_0_transitions = {\n",
    "            0: lambda: [0.5, 0],\n",
    "            1: lambda: [-1, 0],\n",
    "            2: lambda: [[-2, 1],[-0.5, 0]][np.random.choice(2,p=[0.7,0.3])]\n",
    "        }\n",
    "        \n",
    "        state_1_transitions = {\n",
    "            0: lambda: [-2, 1],\n",
    "            1: lambda: [1.2, 1],\n",
    "            2: lambda: [-1.5, 1],\n",
    "            3: lambda: [[-2, 0],[-0.5, 1]][np.random.choice(2,p=[0.8,0.2])]\n",
    "        }\n",
    "        \n",
    "        reward = None\n",
    "        next_state = None\n",
    "        if (self.agent_pos==0):\n",
    "            # Agent is at Casino 1. Use state 0 transitions\n",
    "            reward, next_state = state_0_transitions[action]()\n",
    "        else:\n",
    "            # Agent is at Casino 2. Use state 1 transitions\n",
    "            reward, next_state = state_1_transitions[action]()\n",
    "            \n",
    "        # Transition the agent to the next state\n",
    "        self.agent_pos = next_state\n",
    "        # Return the data as defined by the gym interface\n",
    "        return self._next_observation(), reward, False, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may look complicated, but there's not much going on in here. We defined some objects that display how the environment reacts to the agent's actions. Then, depending on the current state the agent is found, the agent responds and generates the `next_state` and `reward`. Lastly, we return the information the way the gym interface expects us to do. We also defined another function, called `_next_observation`. It's a pretty basic function, that was added mainly to keep our observations consistent. \n",
    "\n",
    "Other remarks about what we return:\n",
    "\n",
    "- the `done` variable is always set to False, since our environment doesn't have any termination state.\n",
    "- the `info` variable is always empty. We do not need to add any auxiliary information with our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset\n",
    "\n",
    "Our reset function will be pretty simple. The only thing we need to reset is the agent position. Our reset function needs to return an observation, so we're reusing the `_next_observation` function declared above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasinosEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Casinos custom environment, built with the gym interface\n",
    "    \"\"\"\n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the agent's position\n",
    "        self.agent_pos = np.random.randint(2)\n",
    "        return self._next_observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render\n",
    "\n",
    "Inside the render function we define how we want to visualize the environment. The gym interface offers a list of modes that people might expect to find in an environment. It is up to us to define how our environment displays, and which modes it supports. The `metadata` global property inside our environment's class contains a list of supported modes, which in our case only contains `human`. The `human` mode means that whatever we render is intended for a human to visualize. We will simply print the agent's current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasinosEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Casinos custom environment, built with the gym interface\n",
    "    \"\"\"\n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            pretty_print_state = {\n",
    "                0: \"Casino 1\",\n",
    "                1: \"Casino 2\"\n",
    "            }\n",
    "            print('Current State: {}'.format(pretty_print_state[self.agent_pos]))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Implementation\n",
    "\n",
    "Now that all of the required functionality has been explained, let's put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CasinosEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Casinos custom environment, built with the gym interface\n",
    "    \"\"\"\n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CasinosEnv, self).__init__()\n",
    "        \n",
    "        self.observation_space = spaces.Discrete(2)\n",
    "        self.action_space = spaces.Tuple((spaces.Discrete(3), spaces.Discrete(4)))\n",
    "        self.agent_pos = np.random.randint(2)\n",
    "        \n",
    "    def _next_observation(self):\n",
    "        return {'state': self.agent_pos}\n",
    "        \n",
    "    def step(self, action):\n",
    "        # dictionary of functions that return [reward, next_state] transitions for each action\n",
    "        state_0_transitions = {\n",
    "            0: lambda: [0.5, 0],\n",
    "            1: lambda: [-1, 0],\n",
    "            2: lambda: [[-2, 1],[-0.5, 0]][np.random.choice(2,p=[0.7,0.3])]\n",
    "        }\n",
    "        \n",
    "        state_1_transitions = {\n",
    "            0: lambda: [-2, 1],\n",
    "            1: lambda: [1.2, 1],\n",
    "            2: lambda: [-1.5, 1],\n",
    "            3: lambda: [[-2, 0],[-0.5, 1]][np.random.choice(2,p=[0.8,0.2])]\n",
    "        }\n",
    "        \n",
    "        reward = None\n",
    "        next_state = None\n",
    "        if (self.agent_pos==0):\n",
    "            # Agent is at Casino 1. Use state 0 transitions\n",
    "            reward, next_state = state_0_transitions[action]()\n",
    "        else:\n",
    "            # Agent is at Casino 2. Use state 1 transitions\n",
    "            reward, next_state = state_1_transitions[action]()\n",
    "            \n",
    "        # Transition the agent to the next state\n",
    "        self.agent_pos = next_state\n",
    "        # Return the data as defined by the gym interface\n",
    "        return self._next_observation(), reward, False, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the agent's position\n",
    "        self.agent_pos = np.random.randint(2)\n",
    "        return self._next_observation()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            pretty_print_state = {\n",
    "                0: \"Casino 1\",\n",
    "                1: \"Casino 2\"\n",
    "            }\n",
    "            print('Current State: {}'.format(pretty_print_state[self.agent_pos]))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Note that some comments were removed for space. Our MDP is now implemented, and we can now interact with it! Let's make a pretty simple agent and do some simulation with it. Our agent will simply take random actions and print which action it took, as well as the reward received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: Casino 2\n",
      "Action taken: Arm 1\n",
      "Reward obtained: -2\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Arm 1\n",
      "Reward obtained: -2\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Arm 2\n",
      "Reward obtained: 1.2\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Arm 2\n",
      "Reward obtained: 1.2\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Arm 3\n",
      "Reward obtained: -1.5\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Take a bus\n",
      "Reward obtained: -2\n",
      "===========================\n",
      "Current State: Casino 1\n",
      "Action taken: Take a bus\n",
      "Reward obtained: -2\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Arm 3\n",
      "Reward obtained: -1.5\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Take a bus\n",
      "Reward obtained: -0.5\n",
      "===========================\n",
      "Current State: Casino 2\n",
      "Action taken: Arm 3\n",
      "Reward obtained: -1.5\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "env = CasinosEnv()\n",
    "curr_state = env.agent_pos\n",
    "steps = 10\n",
    "\n",
    "pretty_print_a_0 = {\n",
    "    0: \"Arm 1\",\n",
    "    1: \"Arm 2\",\n",
    "    2: \"Take a bus\",\n",
    "}\n",
    "\n",
    "pretty_print_a_1 = {\n",
    "    0: \"Arm 1\",\n",
    "    1: \"Arm 2\",\n",
    "    2: \"Arm 3\",\n",
    "    3: \"Take a bus\",\n",
    "}\n",
    "for _ in range(steps):\n",
    "    env.render()\n",
    "    action = env.action_space[curr_state].sample() # Sample an action from the current state\n",
    "    print_a = None\n",
    "    if curr_state==0:\n",
    "        print_a = pretty_print_a_0[action]\n",
    "    else:\n",
    "        print_a = pretty_print_a_1[action]\n",
    "    print(\"Action taken: {}\".format(print_a))\n",
    "    obs, reward, _, _ = env.step(action) # Execute a step, get observation and reward\n",
    "    curr_state = obs['state']\n",
    "    print(\"Reward obtained: {}\".format(reward))\n",
    "    print(\"===========================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Exercises\n",
    "\n",
    "There are many changes we can make to our environment, and here are some simple ones that could be fun to attempt as an exercise:\n",
    "\n",
    "- Make the Arms return random rewards based on a normal distribution, just like the Multi-Armed Bandits **(easy)**\n",
    "- Add some more states **(medium)**\n",
    "- Make the number of states and actions variable, so that the developer can define them when initializing the environment. Make use of the `__init__` function for this. No state should be isolated from each other **(hard)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
