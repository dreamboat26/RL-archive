{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Code Description for First-Time Users\n",
        "\n",
        "This script implements a **Dueling Deep Q-Network (Dueling DQN)** to train an agent to play the Atari game **Breakout** using reinforcement learning. Below is an overview of the key components and their functions:\n",
        "\n",
        "#### 1. **Core Model Architecture (Dueling DQN)**\n",
        "   - The `DuelingDQN` class defines the neural network architecture with:\n",
        "     - **Convolutional Layers** to process image input.\n",
        "     - **Value and Advantage Streams**: These streams separate the estimation of the overall value of a state and the advantages of individual actions, combining them to output action Q-values.\n",
        "\n",
        "#### 2. **Replay Buffer with Prioritization**\n",
        "   - The `PrioritizedReplayBuffer` stores past experiences (state, action, reward, next state, and done flag).\n",
        "   - It samples experiences based on their importance (priority), which accelerates learning by focusing on significant transitions.\n",
        "\n",
        "#### 3. **BreakoutAgent**\n",
        "   - Handles the agent's:\n",
        "     - **Model**: Policy and target Dueling DQN networks.\n",
        "     - **Experience Replay**: Storing and sampling experiences for training.\n",
        "     - **Action Selection**: Epsilon-greedy strategy balances exploration (random actions) and exploitation (choosing the best-known action).\n",
        "     - **Training**: Uses the sampled experiences to compute and minimize the temporal-difference loss.\n",
        "\n",
        "#### 4. **Game Interaction**\n",
        "   - Uses the `ALEInterface` (Atari Learning Environment) to interact with the Breakout game:\n",
        "     - Captures screen images, processes them into grayscale and resized tensors.\n",
        "     - Executes actions using the minimal action set provided by the ALE.\n",
        "\n",
        "#### 5. **Training Process**\n",
        "   - The `train_agent` function:\n",
        "     - Initializes the environment and agent.\n",
        "     - Runs for a specified number of episodes, during which:\n",
        "       - The agent interacts with the environment, storing experiences in the replay buffer.\n",
        "       - Periodically trains the policy network using sampled experiences.\n",
        "       - Adjusts exploration (epsilon) based on recent performance trends.\n",
        "       - Tracks rewards and updates the target network at intervals.\n",
        "     - Optionally renders the game at regular intervals for visualization.\n",
        "\n",
        "#### 6. **Evaluation**\n",
        "   - The `evaluate_agent` function:\n",
        "     - Runs the trained agent in the game environment for a few episodes.\n",
        "     - Measures and displays the performance of the trained agent.\n",
        "\n",
        "#### 7. **Visualization**\n",
        "   - Plots the training rewards over episodes to monitor learning progress.\n",
        "\n",
        "#### 8. **Execution Flow**\n",
        "   - The script trains the agent for 1,000 episodes, plots training progress, and evaluates the agent's performance over 10 evaluation episodes.\n",
        "\n",
        "### Key Notes for First-Time Use\n",
        "- Ensure **Python dependencies** (`numpy`, `torch`, `ale-py`, `opencv`, etc.) are installed.\n",
        "- Verify that the Breakout ROM is correctly loaded by `ale-py`.\n",
        "- For training visualization, the game screen will display (requires a graphical environment).\n",
        "- Training can take considerable time depending on hardware (using a GPU is recommended)."
      ],
      "metadata": {
        "id": "pqzyk5fczx0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "from ale_py import ALEInterface, roms\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Dueling DQN architecture\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        self.fc_value = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        self.fc_advantage = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        value = self.fc_value(conv_out)\n",
        "        advantage = self.fc_advantage(conv_out)\n",
        "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.alpha = alpha\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.priorities = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_priority = max(self.priorities, default=1.0)\n",
        "        self.buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        self.priorities.append(float(max_priority))\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        priorities = np.array(list(self.priorities), dtype=np.float32)\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        indices = random.choices(range(len(self.buffer)), k=batch_size, weights=probabilities)\n",
        "        experiences = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        states = torch.stack([exp.state for exp in experiences])\n",
        "        actions = torch.tensor([exp.action for exp in experiences], dtype=torch.long)\n",
        "        rewards = torch.tensor([exp.reward for exp in experiences], dtype=torch.float)\n",
        "        next_states = torch.stack([exp.next_state for exp in experiences])\n",
        "        dones = torch.tensor([exp.done for exp in experiences], dtype=torch.float)\n",
        "        weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = float(priority.item())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class BreakoutAgent:\n",
        "    def __init__(self, state_shape, n_actions, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.device = device\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.policy_net = DuelingDQN(state_shape, n_actions).to(device)\n",
        "        self.target_net = DuelingDQN(state_shape, n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)\n",
        "        self.memory = PrioritizedReplayBuffer(100000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.target_update = 1000\n",
        "        self.steps = 0\n",
        "\n",
        "        self.frame_skip = 4\n",
        "\n",
        "        # Performance tracking\n",
        "        self.performance_window = deque(maxlen=5)\n",
        "        self.exploration_increase_threshold = -10\n",
        "\n",
        "    def adjust_exploration(self, recent_reward):\n",
        "        \"\"\"Dynamically adjust exploration based on performance trends\"\"\"\n",
        "        self.performance_window.append(recent_reward)\n",
        "\n",
        "        if len(self.performance_window) == self.performance_window.maxlen:\n",
        "            # Calculate the trend in recent performance\n",
        "            performance_trend = sum(y - x for x, y in zip(\n",
        "                self.performance_window,\n",
        "                list(self.performance_window)[1:]\n",
        "            )) / (len(self.performance_window) - 1)\n",
        "\n",
        "            if performance_trend < self.exploration_increase_threshold:\n",
        "                # Bad performance trend - increase exploration\n",
        "                self.epsilon = min(\n",
        "                    self.epsilon_max,\n",
        "                    self.epsilon + 0.1\n",
        "                )\n",
        "                return \"increased\"\n",
        "            elif performance_trend > 0:\n",
        "                # Good performance trend - decrease exploration gradually\n",
        "                self.epsilon = max(\n",
        "                    self.epsilon_min,\n",
        "                    self.epsilon * self.epsilon_decay\n",
        "                )\n",
        "                return \"decreased\"\n",
        "            else:\n",
        "                # Neutral trend - maintain current exploration\n",
        "                return \"maintained\"\n",
        "        return \"initializing\"\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        gray = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
        "        resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        processed = torch.FloatTensor(resized).unsqueeze(0) / 255.0\n",
        "        return processed\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.n_actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = state.unsqueeze(0).to(self.device)\n",
        "            q_values = self.policy_net(state)\n",
        "            return q_values.max(1)[1].item()\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        weights = weights.to(self.device)\n",
        "\n",
        "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
        "        expected_q_values = rewards + (self.gamma * next_q_values * (1.0 - dones))\n",
        "\n",
        "        loss = nn.MSELoss()(current_q_values, expected_q_values.unsqueeze(1))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        td_errors = torch.abs(current_q_values - expected_q_values.unsqueeze(1)).detach().cpu().numpy()\n",
        "        self.memory.update_priorities(indices, td_errors)\n",
        "\n",
        "        if self.steps % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.steps += 1\n",
        "        return loss.item()\n",
        "\n",
        "def train_agent(episodes=1000, render_frequency=100):\n",
        "    ale = ALEInterface()\n",
        "    ale.setInt('random_seed', 123)\n",
        "    ale.setBool('sound', False)\n",
        "    ale.setBool('display_screen', True)\n",
        "    ale.setFloat('repeat_action_probability', 0.0)\n",
        "    ale.loadROM(roms.get_rom_path(\"breakout\"))\n",
        "\n",
        "    actions = ale.getMinimalActionSet()\n",
        "    state_shape = (1, 84, 84)\n",
        "    agent = BreakoutAgent(state_shape, len(actions))\n",
        "\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in tqdm(range(episodes)):\n",
        "        ale.reset_game()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        state = agent.preprocess_state(ale.getScreenRGB())\n",
        "\n",
        "        while not done:\n",
        "            action_idx = agent.select_action(state)\n",
        "            reward = 0\n",
        "\n",
        "            for _ in range(agent.frame_skip):\n",
        "                reward += ale.act(actions[action_idx])\n",
        "                if ale.game_over():\n",
        "                    done = True\n",
        "                    break\n",
        "\n",
        "            if not done:\n",
        "                next_state = agent.preprocess_state(ale.getScreenRGB())\n",
        "            else:\n",
        "                next_state = state.clone()\n",
        "\n",
        "            agent.memory.push(state, action_idx, float(reward), next_state, float(done))\n",
        "            loss = agent.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if episode % render_frequency == 0:\n",
        "                screen = ale.getScreenRGB()\n",
        "                cv2.imshow('Breakout', cv2.cvtColor(screen, cv2.COLOR_RGB2BGR))\n",
        "                cv2.waitKey(1)\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        exploration_status = agent.adjust_exploration(total_reward)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, \"\n",
        "                  f\"Epsilon: {agent.epsilon:.2f}, Exploration: {exploration_status}\")\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    return agent, episode_rewards\n",
        "\n",
        "def evaluate_agent(agent, n_episodes=10):\n",
        "    ale = ALEInterface()\n",
        "    ale.setBool('sound', False)\n",
        "    ale.setBool('display_screen', True)\n",
        "    ale.loadROM(roms.get_rom_path(\"breakout\"))\n",
        "    actions = ale.getMinimalActionSet()\n",
        "\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        ale.reset_game()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = agent.preprocess_state(ale.getScreenRGB())\n",
        "            action_idx = agent.select_action(state)\n",
        "\n",
        "            reward = 0\n",
        "            for _ in range(agent.frame_skip):\n",
        "                reward += ale.act(actions[action_idx])\n",
        "                if ale.game_over():\n",
        "                    done = True\n",
        "                    break\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            screen = ale.getScreenRGB()\n",
        "            cv2.imshow('Breakout', cv2.cvtColor(screen, cv2.COLOR_RGB2BGR))\n",
        "            cv2.waitKey(1)\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1} Reward: {total_reward}\")\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    return total_rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trained_agent, training_rewards = train_agent(episodes=1000)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(training_rewards)\n",
        "    plt.title(\"Training Progress\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.show()\n",
        "\n",
        "    evaluation_rewards = evaluate_agent(trained_agent)\n",
        "    print(f\"Average Evaluation Reward: {np.mean(evaluation_rewards):.2f}\")"
      ],
      "metadata": {
        "id": "F_aW3ftjzvEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Description for First-Time Users: Contrasting Simple vs Complex RL Implementations\n",
        "\n",
        "This script implements a **complex reinforcement learning (RL) system** using a **swarm-based DQN approach**. It contrasts with a simpler RL implementation by introducing advanced features like swarm collaboration, probabilistic layers, and prioritized replay, applied to the Atari game **Q*bert**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Simple RL Implementation Overview**\n",
        "- **Goal**: Train a single agent using a basic Dueling DQN to play a game.\n",
        "- **Key Features**:\n",
        "  - **Dueling DQN**: Splits Q-value computation into value and advantage streams for better performance.\n",
        "  - **Prioritized Replay Buffer**: Focuses training on important experiences.\n",
        "  - **Action Selection**: Uses an epsilon-greedy strategy for exploration and exploitation.\n",
        "- **Limitations**:\n",
        "  - Single-agent setup lacks collaborative behavior.\n",
        "  - Exploration strategies are limited to basic epsilon-greedy methods.\n",
        "  - Model focuses purely on individual performance with no inter-agent learning.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Complex RL Implementation (This Script)**\n",
        "- **Goal**: Train a **swarm of agents** collaboratively to solve the same task, leveraging advanced exploration and probabilistic enhancements.\n",
        "- **Key Features**:\n",
        "\n",
        "#### **Advanced Neural Network Enhancements**\n",
        "- **Gaussian Probability Layer (GPL)**: Introduces controlled noise to promote robust learning through probabilistic \"twistronics.\"\n",
        "- **NoisyLinear Layer**: Replaces standard linear layers to dynamically balance exploration and exploitation by adding learnable noise.\n",
        "\n",
        "#### **Swarm Collaboration**\n",
        "- **SwarmDQN**: Multiple agents (swarm members) independently learn and contribute to a shared **global best policy**.\n",
        "- Each agent tracks and updates its **personal best reward and weights**, enabling decentralized learning.\n",
        "- A **global best reward and weights** are updated based on the highest-performing agent, promoting knowledge sharing.\n",
        "\n",
        "#### **Environment Interaction**\n",
        "- Uses **stacked frames** to capture temporal information.\n",
        "- Maintains a **Prioritized Replay Buffer**, shared among all swarm members, to store and sample experiences for training.\n",
        "\n",
        "#### **Exploration and Reward Adjustment**\n",
        "- **Action Suggestion**: Combines epsilon-greedy exploration with a mechanism where agents \"suggest\" actions based on personal policies, weighted by their expertise.\n",
        "- **Reward Calculation**: Rewards are adjusted based on the alignment of a taken action with the suggested action, encouraging coordinated behavior.\n",
        "\n",
        "#### **Dynamic Member Updates**\n",
        "- Each swarm member independently trains on minibatches, using:\n",
        "  - **Temporal Difference Learning**: Updates Q-values based on predictions and actual rewards.\n",
        "  - **Shared Replay Buffer**: Ensures consistent experiences across the swarm.\n",
        "\n",
        "#### **Visualization and Metrics**\n",
        "- Tracks:\n",
        "  - **Overall training progress** for the swarm.\n",
        "  - **Performance of individual swarm members**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Execution Flow**\n",
        "1. **Swarm Initialization**:\n",
        "   - A specified number of agents (`swarm_size`) is initialized with identical architectures.\n",
        "   - Agents share access to a centralized replay buffer but operate semi-independently.\n",
        "\n",
        "2. **Training Loop**:\n",
        "   - For each episode, a random swarm member interacts with the environment:\n",
        "     - Selects actions using both personal policy and shared suggestions.\n",
        "     - Updates the replay buffer with experiences.\n",
        "     - Trains on minibatches from the shared replay buffer.\n",
        "   - Updates personal and global best policies based on performance.\n",
        "\n",
        "3. **Evaluation and Visualization**:\n",
        "   - After training, plots are generated to compare:\n",
        "     - **Overall swarm progress**.\n",
        "     - **Individual performance** of each swarm member.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points of Contrast**\n",
        "| Feature                            | **Simple RL Implementation**          | **Complex RL Implementation**        |\n",
        "|------------------------------------|---------------------------------------|---------------------------------------|\n",
        "| **Model**                          | Single Dueling DQN                    | Swarm of Dueling DQNs                |\n",
        "| **Exploration**                    | Epsilon-greedy                        | Noisy layers and GPL for enhanced exploration |\n",
        "| **Collaboration**                  | None                                  | Swarm collaboration with global policy sharing |\n",
        "| **Replay Buffer**                  | Individual, prioritized               | Centralized, prioritized              |\n",
        "| **Learning Strategy**              | Independent learning                  | Collaborative learning                |\n",
        "| **Target Environment**             | Single agent in Breakout              | Multi-agent system in Q*bert          |\n",
        "\n",
        "This complex RL script demonstrates how advanced exploration strategies, probabilistic mechanisms, and multi-agent collaboration can significantly enhance learning efficiency and performance in RL tasks."
      ],
      "metadata": {
        "id": "2nPZb_aB0Vz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_OJZCJP_MPq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "from ale_py import ALEInterface, roms\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# Define Gaussian Probability Layer (GPL) for probabilistic \"twistronics\" effect\n",
        "class GaussianProbabilityLayer(nn.Module):\n",
        "    def __init__(self, std_dev=0.1):\n",
        "        super(GaussianProbabilityLayer, self).__init__()\n",
        "        self.std_dev = std_dev\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            noise = torch.randn_like(x) * self.std_dev\n",
        "            return x + noise\n",
        "        return x\n",
        "\n",
        "# Define NoisyLinear for exploration-exploitation tradeoff\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std_init=0.5):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
        "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
        "        self.std_init = std_init\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / np.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
        "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
        "        else:\n",
        "            weight = self.weight_mu\n",
        "            bias = self.bias_mu\n",
        "        return nn.functional.linear(input, weight, bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def _scale_noise(size):\n",
        "        x = torch.randn(size)\n",
        "        return x.sign().mul_(x.abs().sqrt_())\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Calculate the correct conv output size\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        # Apply GaussianProbabilityLayer between convolutional and fully connected layers\n",
        "        self.gpl1 = GaussianProbabilityLayer(std_dev=0.1)\n",
        "\n",
        "        # Value stream\n",
        "        self.fc_value = nn.Sequential(\n",
        "            NoisyLinear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            self.gpl1,\n",
        "            NoisyLinear(512, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream\n",
        "        self.fc_advantage = nn.Sequential(\n",
        "            NoisyLinear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            GaussianProbabilityLayer(std_dev=0.05),\n",
        "            NoisyLinear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        # Generate a dummy input to pass through conv layers and calculate output size\n",
        "        dummy_input = torch.zeros(1, *shape)\n",
        "        o = self.conv(dummy_input)\n",
        "        return int(np.prod(o.size()[1:]))  # Only multiply the feature dimensions, not batch\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add batch dimension if input is a single sample\n",
        "        if len(x.size()) == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Pass through convolutional layers\n",
        "        conv_out = self.conv(x)\n",
        "\n",
        "        # Flatten while preserving batch dimension\n",
        "        batch_size = conv_out.size(0)\n",
        "        conv_out = conv_out.view(batch_size, -1)\n",
        "\n",
        "        # Apply GPL and compute value and advantage streams\n",
        "        conv_out = self.gpl1(conv_out)\n",
        "        value = self.fc_value(conv_out)\n",
        "        advantage = self.fc_advantage(conv_out)\n",
        "\n",
        "        # Combine value and advantage using dueling architecture formula\n",
        "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "# Define the Experience tuple for replay buffer\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "# Define the Prioritized Replay Buffer\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.alpha = alpha\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.priorities = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_priority = max(self.priorities, default=1.0)\n",
        "        self.buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        self.priorities.append(float(max_priority))\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return None\n",
        "\n",
        "        priorities = np.array(list(self.priorities), dtype=np.float32)\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        indices = random.choices(range(len(self.buffer)), k=batch_size, weights=probabilities)\n",
        "        experiences = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        states = torch.stack([exp.state for exp in experiences])\n",
        "        actions = torch.tensor([exp.action for exp in experiences], dtype=torch.long)\n",
        "        rewards = torch.tensor([exp.reward for exp in experiences], dtype=torch.float)\n",
        "        next_states = torch.stack([exp.next_state for exp in experiences])\n",
        "        dones = torch.tensor([exp.done for exp in experiences], dtype=torch.float)\n",
        "        weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = float(priority.item())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Preprocess the state from the environment\n",
        "def preprocess_state(state):\n",
        "    gray = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
        "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "    processed = torch.tensor(resized, dtype=torch.float32).unsqueeze(0) / 255.0\n",
        "    return processed\n",
        "\n",
        "# Define the Swarm Member for individual DQN agents\n",
        "class SwarmMember:\n",
        "    def __init__(self, state_shape, n_actions, device, id):\n",
        "        self.id = id\n",
        "        self.device = device\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.policy_net = DuelingDQN(state_shape, n_actions).to(device)\n",
        "        self.target_net = DuelingDQN(state_shape, n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.9995\n",
        "\n",
        "        self.personal_best_reward = float('-inf')\n",
        "        self.personal_best_weights = copy.deepcopy(self.policy_net.state_dict())\n",
        "\n",
        "    def update_personal_best(self, episode_reward):\n",
        "        if episode_reward > self.personal_best_reward:\n",
        "            self.personal_best_reward = episode_reward\n",
        "            self.personal_best_weights = copy.deepcopy(self.policy_net.state_dict())\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def get_suggested_action(self, state):\n",
        "        # Epsilon-greedy policy for exploration\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "# Define the Swarm DQN with Qbert environment\n",
        "class SwarmDQN:\n",
        "    def __init__(self, state_shape, n_actions, swarm_size=5, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.device = device\n",
        "        self.swarm_size = swarm_size\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.swarm = [SwarmMember(state_shape, n_actions, device, i) for i in range(swarm_size)]\n",
        "\n",
        "        self.memory = PrioritizedReplayBuffer(100000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        self.global_best_reward = float('-inf')\n",
        "        self.global_best_weights = copy.deepcopy(self.swarm[0].policy_net.state_dict())\n",
        "\n",
        "    def select_action(self, state, member_idx):\n",
        "        member = self.swarm[member_idx]\n",
        "\n",
        "        suggested_action = member.get_suggested_action(state.squeeze().cpu())\n",
        "        if random.random() < member.epsilon:\n",
        "            if suggested_action is not None and random.random() < 0.7:\n",
        "                return suggested_action\n",
        "            return random.randrange(self.n_actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = member.policy_net(state)\n",
        "                if suggested_action is not None:\n",
        "                    q_values[0][suggested_action] += 0.1\n",
        "                return torch.argmax(q_values).item()\n",
        "\n",
        "    def calculate_reward(self, raw_reward, suggested_action, taken_action):\n",
        "        reward = raw_reward\n",
        "        if suggested_action is not None and suggested_action == taken_action:\n",
        "            reward += 0.1\n",
        "        return reward\n",
        "\n",
        "    def update_member(self, member_idx, batch):\n",
        "        member = self.swarm[member_idx]\n",
        "        states, actions, rewards, next_states, dones, indices, weights = batch\n",
        "\n",
        "        states = states.view(self.batch_size, -1, 84, 84).to(self.device)\n",
        "        next_states = next_states.view(self.batch_size, -1, 84, 84).to(self.device)\n",
        "\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        weights = weights.to(self.device)\n",
        "\n",
        "        # Forward pass only\n",
        "        with torch.no_grad():\n",
        "            current_q_values = member.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "            next_actions = member.policy_net(next_states).max(1)[1]\n",
        "            next_q_values = member.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "            target_q_values = rewards + (self.gamma * next_q_values * (1.0 - dones))\n",
        "\n",
        "        td_errors = torch.abs(current_q_values - target_q_values.unsqueeze(1)).detach().cpu().numpy()\n",
        "        self.memory.update_priorities(indices, td_errors.squeeze())\n",
        "\n",
        "    def update_global_best(self, member_idx, episode_reward):\n",
        "        if episode_reward > self.global_best_reward:\n",
        "            self.global_best_reward = episode_reward\n",
        "            self.global_best_weights = copy.deepcopy(self.swarm[member_idx].policy_net.state_dict())\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Train the Swarm Agent for Qbert\n",
        "def train_swarm_agent(episodes=1000, render_frequency=100, score_threshold=50, hit_threshold=20):\n",
        "    ale = ALEInterface()\n",
        "    ale.setInt('random_seed', 123)\n",
        "    ale.setBool('sound', False)\n",
        "    ale.setBool('display_screen', True)\n",
        "    ale.setFloat('repeat_action_probability', 0.0)\n",
        "    ale.loadROM(roms.get_rom_path(\"qbert\"))\n",
        "\n",
        "    actions = ale.getMinimalActionSet()\n",
        "    state_shape = (4, 84, 84)\n",
        "    swarm = SwarmDQN(state_shape, len(actions))\n",
        "\n",
        "    episode_rewards = []\n",
        "    swarm_rewards = [[] for _ in range(swarm.swarm_size)]\n",
        "\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        member_idx = random.randrange(swarm.swarm_size)\n",
        "        member = swarm.swarm[member_idx]\n",
        "\n",
        "        ale.reset_game()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        state_stack = deque([preprocess_state(ale.getScreenRGB()) for _ in range(4)], maxlen=4)\n",
        "\n",
        "        while not done:\n",
        "            stacked_state = torch.cat(list(state_stack), dim=0).unsqueeze(0).to(swarm.device)\n",
        "\n",
        "            action_idx = swarm.select_action(stacked_state, member_idx)\n",
        "            reward = 0\n",
        "\n",
        "            for _ in range(4):\n",
        "                reward += ale.act(actions[action_idx])\n",
        "                if ale.game_over():\n",
        "                    done = True\n",
        "                    break\n",
        "\n",
        "            next_state = preprocess_state(ale.getScreenRGB()) if not done else state_stack[-1].clone()\n",
        "            state_stack.append(next_state)\n",
        "\n",
        "            stacked_next_state = torch.cat(list(state_stack), dim=0).unsqueeze(0).to(swarm.device)\n",
        "\n",
        "            swarm.memory.push(stacked_state, action_idx, float(reward), stacked_next_state, float(done))\n",
        "\n",
        "            if len(swarm.memory) >= swarm.batch_size:\n",
        "                batch = swarm.memory.sample(swarm.batch_size)\n",
        "                if batch is not None:\n",
        "                    swarm.update_member(member_idx, batch)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if episode % render_frequency == 0:\n",
        "                screen = ale.getScreenRGB()\n",
        "                cv2.imshow('Qbert', cv2.cvtColor(screen, cv2.COLOR_RGB2BGR))\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        swarm_rewards[member_idx].append(total_reward)\n",
        "\n",
        "        personal_best_updated = member.update_personal_best(total_reward)\n",
        "        if personal_best_updated:\n",
        "            swarm.update_global_best(member_idx, total_reward)\n",
        "\n",
        "        member.epsilon = max(member.epsilon_min, member.epsilon * member.epsilon_decay)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Member {member_idx} Epsilon: {member.epsilon:.2f}\")\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    return swarm, episode_rewards, swarm_rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        swarm, episode_rewards, swarm_rewards = train_swarm_agent()\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(episode_rewards)\n",
        "        plt.title(\"Overall Training Progress\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        for i, rewards in enumerate(swarm_rewards):\n",
        "            plt.plot(rewards, label=f\"Member {i}\")\n",
        "        plt.title(\"Individual Swarm Member Performance\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)"
      ]
    }
  ]
}