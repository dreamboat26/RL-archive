{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Reinforcement_Learning_Part_10_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeiTEgZAQWSR"
      },
      "source": [
        "This is the Part-10 of the Deep Reinforcement Learning Notebook series. In this Notebook I have introduced Asynchronous Advantage Actor-Critic Algorithm (A3C).\n",
        "\n",
        "\n",
        "The Notebook series is about Deep RL algorithms so it excludes all other techniques that can be used to learn functions in reinforcement learning and also the Notebook Series is not exhaustive i.e. it contains the most widely used Deep RL algorithms only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfvKOMjq3Hlr"
      },
      "source": [
        "'''\n",
        "  Few things in this notebook are taken from https://blog.tensorflow.org/2018/07/deep-reinforcement-learning-keras-eager-execution.html\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0pMHwtzQ8Pi"
      },
      "source": [
        "##What is Asynchronous Advantage Actor Critic?\n",
        "\n",
        "Asynchronous Advantage Actor Critic is quite a mouthful! Let’s start by breaking down the name, and then the mechanics behind the algorithm itself.\n",
        "\n",
        "Asynchronous: The algorithm is an asynchronous algorithm where multiple worker agents are trained in parallel, each with their own copy of the model and environment. This allows our algorithm to not only train faster as more workers are training in parallel, but also to attain a more diverse training experience as each workers’ experience is independent.\n",
        "\n",
        "Advantage: Advantage is a metric to judge both how good its actions were, but also how they turned out. This allows the algorithm to focus on where the network’s predictions were lacking. Intuitively, this allows us to measure the advantage of taking action, a, over following the policy π at the given time step.\n",
        "\n",
        "Actor-Critic: The Actor-Critic aspect of the algorithm uses an architecture that shares layers between the policy and value function.\n",
        "\n",
        "\n",
        "One of the most exciting thing about the paper, is that you don’t need to rely on a GPU for speed. In fact, the whole idea is to use multiple cores of a CPU, run in parallel, which gives a speedup proportional to the number of cores used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImPrODOFYPy8"
      },
      "source": [
        "A2C and A3C differ only in asynchronous part. A3C consists of multiple independent agents(networks) with their own weights, who interact with a different copy of the environment in parallel. Thus, they can explore a bigger part of the state-action space in much less time "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLvjYclQUYFc"
      },
      "source": [
        "##Asynchronous Advantage Actor Critic Algorithm\n",
        "Each worker performs the following workflow cycle:\n",
        "\n",
        "1) Workers reset to global network\n",
        "\n",
        "2) Workers interact with the environment\n",
        "\n",
        "3) Workers calculate value and policy loss\n",
        "\n",
        "4) Workers get gradients from losses\n",
        "\n",
        "5) Workers update the global network with the gradients\n",
        "\n",
        "6) Repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jlhMVJOYIO2"
      },
      "source": [
        "##IMPLEMENTING Asynchronous Advantage Actor Critic Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBYOhWYKYZ2u"
      },
      "source": [
        "Below code setups the environment required to run and record the game and also loads the required library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5cnCfu-1MK8"
      },
      "source": [
        "!sudo apt install cmake libboost-all-dev libsdl2-dev libfreetype6-dev libgl1-mesa-dev libglu1-mesa-dev libpng-dev libjpeg-dev libbz2-dev libfluidsynth-dev libgme-dev libopenal-dev zlib1g-dev timidity tar nasm\n",
        "!pip3 install vizdoom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z_v42zXYaNO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Dropout,Conv2D, Flatten,BatchNormalization ,Activation,Input\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "import gym\n",
        "import numpy as np\n",
        "import threading\n",
        "from queue import Queue\n",
        "import imageio\n",
        "import multiprocessing\n",
        "from vizdoom import *\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.utils import normalize as normal_values\n",
        "import cv2\n",
        "from gym import logger as gymlogger\n",
        "from skimage.transform import resize\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        " %matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t478-SH41Too"
      },
      "source": [
        "This part setups the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExOGX-Uv1OIi"
      },
      "source": [
        "def get_game():\n",
        "  game= DoomGame()\n",
        "  game.load_config(\"defend_the_center.cfg\")\n",
        "  game.set_doom_scenario_path(\"defend_the_center.wad\")\n",
        "  \n",
        "  game.init()\n",
        "  return game"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIjnRV7Vx39k"
      },
      "source": [
        "def record(episode,\n",
        "           episode_reward,\n",
        "           worker_idx,\n",
        "           global_ep_reward,\n",
        "           result_queue):\n",
        "  \"\"\"Helper function to store score and print statistics.\n",
        "  Args:\n",
        "    episode: Current episode\n",
        "    episode_reward: Reward accumulated over the current episode\n",
        "    worker_idx: Which thread (worker)\n",
        "    global_ep_reward: The moving average of the global reward\n",
        "    result_queue: Queue storing the moving average of the scores\n",
        "    total_loss: The total loss accumualted over the current episode\n",
        "    num_steps: The number of steps the episode took to complete\n",
        "  \"\"\"\n",
        "  if global_ep_reward == 0:\n",
        "    global_ep_reward = episode_reward\n",
        "  else:\n",
        "    global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n",
        "  print(\n",
        "      f\"Episode: {episode} | \"\n",
        "      f\"Moving Average Reward: {int(global_ep_reward)} | \"\n",
        "      f\"Episode Reward: {int(episode_reward)} | \"\n",
        "      f\"Worker: {worker_idx}\"\n",
        "  )\n",
        "  result_queue.put(global_ep_reward)\n",
        "  return global_ep_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgA07bGH1Z3g"
      },
      "source": [
        "Defining the A3C Worker Class. At initiation, the object sets a few parameters like, action and state space,create Actor,Critic,and remember(that records the observations of each step.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNZNybFc2KIo"
      },
      "source": [
        "class A3C_worker(threading.Thread):\n",
        "\n",
        "  # Set up global variables across different threads\n",
        "  global_episode = 0\n",
        "  # Moving average reward\n",
        "  global_moving_average_reward = 0\n",
        "  best_score = 0\n",
        "  save_lock = threading.Lock()\n",
        "\n",
        "  def __init__(self,global_model,idx,res_queue,no_of_episodes):\n",
        "    super(A3C_worker, self).__init__()\n",
        "    self.result_queue = res_queue\n",
        "    self.env = get_game()\n",
        "    self.no_of_episodes = no_of_episodes\n",
        "    self.worker_idx = idx\n",
        "    self.global_Actor_model = global_model\n",
        "    self.state_shape= 100,160, 4 # the state space\n",
        "    self.action_shape = 3 # the action space\n",
        "    self.actions_=np.identity(self.action_shape,dtype=bool).tolist()\n",
        "    self.gamma=[0.99] # decay rate of past observations\n",
        "    self.learning_rate= 1e-5 # learning rate in deep learning\n",
        "    self.lambda_=0.90       #λ is a hyperparameter for GAE(Generalized Advantage Estimation)\n",
        "    self.alpha=1e-4\n",
        "    self.epsilon = 1.0\n",
        "  \n",
        "    self.Actor_model=self._create_model('Actor')    #Target Model is model used to calculate target values\n",
        "    self.Critic_model=self._create_model('Critic')  #Training Model is model to predict q-values to be used.\n",
        "   \n",
        "        # record observations\n",
        "    self.states=[]\n",
        "    self.rewards=[]\n",
        "    self.actions=[]\n",
        "    self.last_state=np.zeros((1,100,160,4))\n",
        "\n",
        "  def _create_model(self,model_type):\n",
        "    ''' builds the model using keras'''\n",
        "    inputs=Input(shape=(self.state_shape))\n",
        "    layer_1=(Conv2D(32, (8, 8),strides=(4, 4)))(inputs)  \n",
        "    layer_2=(BatchNormalization())(layer_1)\n",
        "    layer_3=(Activation('relu'))(layer_2)\n",
        "    layer_4=(Conv2D(64, (4, 4),strides=(2, 2)))(layer_3)\n",
        "    layer_5=(BatchNormalization())(layer_4)\n",
        "    layer_6=(Activation('relu'))(layer_5)\n",
        "    layer_7=(Conv2D(128, (4, 4),strides=(2, 2)))(layer_6)\n",
        "    layer_8=(BatchNormalization())(layer_7)\n",
        "    layer_9=(Activation('relu'))(layer_8)\n",
        "    layer_10=(Flatten())(layer_9)\n",
        "    layer_11=(Dense(512))(layer_10)\n",
        "    layer_12=(Activation('relu'))(layer_11)\n",
        "    layer_13=(Dense(self.action_shape))(layer_12)\n",
        "    layer_14=(Activation('softmax'))(layer_13)    \n",
        "    layer_15=(Dense(1))(layer_12)\n",
        "\n",
        "    if model_type=='Actor':\n",
        "      model=Model(inputs,layer_14)\n",
        "    else:\n",
        "       model=Model(inputs,layer_15)\n",
        "    model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuFQwVhO2BWb"
      },
      "source": [
        "This is the preprocessing we do to the image we obtained by interacting with the environment. Here I have done grayscaling and also cropped the image to to speed up the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUsroV0x2A-I"
      },
      "source": [
        "  def get_crop_and_grayscale_frame(self,frame):\n",
        "    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame=frame[40:,:]\n",
        "    frame=frame/255.\n",
        "    frame= resize(frame,(100,160))\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viB6wZAB2Qrr"
      },
      "source": [
        "Action Selection\n",
        "\n",
        "The get_action method guides its action choice. It uses the neural network to generate a normalized probability distribution for a given state. Then, it samples its next action from this distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xamiXJuB2T9C"
      },
      "source": [
        "  def get_action(self, state,status='Training'):\n",
        "\n",
        "    '''samples the next action based on the policy probabilty distribution \n",
        "      of the actions'''\n",
        "    if status=='Testing':\n",
        "        return np.argmax(self.Actor_model.predict(state).flatten())\n",
        "\n",
        "    action_probability_distribution=self.Actor_model.predict(state).flatten()    \n",
        "    action = np.random.choice([0,1,2],p=action_probability_distribution.ravel())\n",
        "\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjyycd2h2WcE"
      },
      "source": [
        "The remember method records the observations of each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeDd6PTI2Wzp"
      },
      "source": [
        "  def remember(self, state, reward,action,last_state,done):\n",
        "    '''stores observations'''\n",
        "    self.states.append(state)\n",
        "    self.rewards.append(reward)\n",
        "    self.actions.append(action)\n",
        "    if done:\n",
        "      self.last_state[0]=last_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYFwNda92bGZ"
      },
      "source": [
        "The get_GAEs method calculates Generalized advantage estimation (GAE) which later is used to calculate Advantage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpL3SzaZ2bcR"
      },
      "source": [
        "  def get_GAEs(self,v_preds):\n",
        "    '''\n",
        "    Advantage Estimation with GAE\n",
        "    '''\n",
        "    gaes = np.zeros((len(self.rewards),1))\n",
        "    future_gae = 0.0\n",
        "    for t in reversed(range(len(self.rewards))):\n",
        "      delta = self.rewards[t] + np.asarray(self.gamma) * v_preds[t + 1] - v_preds[t]\n",
        "      gaes[t] = future_gae = delta + np.asarray(self.gamma) * np.asarray(self.lambda_) *np.asarray(future_gae)\n",
        "    return gaes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_lv5eZk2f_E"
      },
      "source": [
        "Updating the models\n",
        "\n",
        "The update_models method calculates gradients w.r.t local network and update global network and also sets weights of local network to global network  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB-kcQ0W2ftb"
      },
      "source": [
        "  def update_models(self):\n",
        "    '''\n",
        "    Updates the network.\n",
        "    '''\n",
        "    #get V_preds and V_tar from critic model\n",
        "    print('updating')\n",
        "    states = (np.array(self.states)).reshape((-1,100,160,4))\n",
        "    actions = (np.array(self.actions)).reshape((-1,1))\n",
        "   \n",
        "    V_s = self.Critic_model.predict(states)\n",
        "    V_last_state = self.Critic_model.predict(self.last_state)\n",
        "    v_all = np.concatenate((V_s,V_last_state),axis=0)\n",
        "\n",
        "    Advantages=self.get_GAEs(v_all) #Calculating the Advantage\n",
        "  \n",
        "    critic_targets = Advantages +  V_s\n",
        "    \n",
        "    self.Critic_model.fit(states, critic_targets,epochs=3) #Training the Local Critic Model\n",
        "    def train_step(states,Advantages,actions):\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=3e-3)\n",
        "      with tf.GradientTape() as tape:\n",
        "        Advantages=tf.stop_gradient(Advantages)\n",
        "        logits=self.Actor_model(states)\n",
        "        negative_likelihoods = tf.nn.softmax_cross_entropy_with_logits(labels=actions,logits=logits)\n",
        "        weighted_negative_likelihoods = tf.multiply(negative_likelihoods,Advantages)\n",
        "        loss = tf.reduce_mean(weighted_negative_likelihoods)\n",
        "      grads = tape.gradient(loss,self.Actor_model.trainable_variables)\n",
        "      grads = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in grads]\n",
        "      optimizer.apply_gradients(zip(grads, self.global_Actor_model.trainable_variables))    #Training the Global Actor Model\n",
        "      self.Actor_model.set_weights(self.global_Actor_model.get_weights())                   #Setting the local Actor Model weights to global Newtork\n",
        "    train_step(tf.cast(states,tf.float32),tf.cast(Advantages,tf.float32),tf.cast(actions,tf.float32))  \n",
        "    self.states=[];self.rewards=[];self.actions=[];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Yjc7jI27BC"
      },
      "source": [
        "Training the model \n",
        "\n",
        "\n",
        "This method creates a training environment for the model. Iterating through a set number of episodes, it uses the model to sample actions and play them. When such a timestep ends, the model is using the observations to update the policy. We know that in a dynamic game we cannot predict action based on 1 observation(which is 1 frame of the game in this case) so we will use a stack of 4 frames to predict the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfC8N6Ag1ZiQ"
      },
      "source": [
        "  def run(self):\n",
        "    while A3C_worker.global_episode < self.no_of_episodes:\n",
        "      # each episode is a new game env\n",
        "      self.env.new_episode();\n",
        "      state=self.env.get_state().screen_buffer\n",
        "      done=False\n",
        "      state= self.get_crop_and_grayscale_frame(state)\n",
        "      stacked_frames = np.stack((state,state,state,state),axis=2)\n",
        "      state_ = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2])\n",
        "      episode_reward=0 #record episode reward\n",
        "      print(\"Episode Started\")\n",
        "      while not done:\n",
        "        # play an action and record the game state & reward per episode\n",
        "        action=self.get_action(state_)\n",
        "        reward=self.env.make_action(self.actions_[action])\n",
        "        done = self.env.is_episode_finished()\n",
        "        if not done:\n",
        "          next_state = self.env.get_state().screen_buffer\n",
        "          next_state = self.get_crop_and_grayscale_frame(next_state)\n",
        "          next_state_ = next_state.reshape(1,next_state.shape[0],next_state.shape[1],1)\n",
        "          next_state_ = np.append(next_state_, state_[:, :, :, :3], axis=3)\n",
        "        else:\n",
        "          next_state_ = state_\n",
        "        if self.env.is_player_dead():\n",
        "          reward=reward-1\n",
        "        self.remember(state_, reward,action,next_state_,done)\n",
        "        state_=next_state_\n",
        "        episode_reward+=reward\n",
        "      self.update_models()\n",
        "      if A3C_worker.global_episode%50==0 and A3C_worker.global_episode!=0:\n",
        "        self.evaluate(A3C_worker.global_episode,self.worker_idx)\n",
        "      self.epsilon = self.epsilon - 0.01\n",
        "      A3C_worker.global_moving_average_reward = record(self.epsilon,A3C_worker.global_episode, episode_reward, self.worker_idx,A3C_worker.global_moving_average_reward, self.result_queue)\n",
        "      if episode_reward > A3C_worker.best_score:\n",
        "        with A3C_worker.save_lock:\n",
        "          print(\"Saving best model\", \"Episode score: {}\".format(episode_reward))\n",
        "          self.global_Actor_model.save('model_{}_{}.h5'.format(self.worker_idx,A3C_worker.global_episode))\n",
        "          A3C_worker.best_score = episode_reward\n",
        "      A3C_worker.global_episode += 1\n",
        "    self.result_queue.put(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLDqSa3X3Oyr"
      },
      "source": [
        "Defining the A3C Global Class. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_jQFzH31os1"
      },
      "source": [
        "class A3C_Global_Network():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.state_shape= 100,160, 4 # the state space\n",
        "    self.action_shape = 3 # the action space\n",
        "    self.global_model = self.create_global_actor()\n",
        "\n",
        "  def create_global_actor(self):\n",
        "    ''' builds the model using keras'''\n",
        "    inputs=Input(shape=(self.state_shape))\n",
        "    layer_1=(Conv2D(32, (8, 8),strides=(4, 4)))(inputs)  \n",
        "    layer_2=(BatchNormalization())(layer_1)\n",
        "    layer_3=(Activation('relu'))(layer_2)\n",
        "    layer_4=(Conv2D(64, (4, 4),strides=(2, 2)))(layer_3)\n",
        "    layer_5=(BatchNormalization())(layer_4)\n",
        "    layer_6=(Activation('relu'))(layer_5)\n",
        "    layer_7=(Conv2D(128, (4, 4),strides=(2, 2)))(layer_6)\n",
        "    layer_8=(BatchNormalization())(layer_7)\n",
        "    layer_9=(Activation('relu'))(layer_8)\n",
        "    layer_10=(Flatten())(layer_9)\n",
        "    layer_11=(Dense(512))(layer_10)\n",
        "    layer_12=(Activation('relu'))(layer_11)\n",
        "    layer_13=(Dense(self.action_shape))(layer_12)\n",
        "    layer_14=(Activation('softmax'))(layer_13)\n",
        "\n",
        "    model=Model(inputs,layer_14)\n",
        "   \n",
        "    return model\n",
        "  \n",
        "  def train(self,no_of_episodes):\n",
        "    res_queue  = Queue()\n",
        "    workers = [A3C_worker(self.global_model,i,res_queue,no_of_episodes) for i in range(multiprocessing.cpu_count())]\n",
        "    for i, worker in enumerate(workers):\n",
        "      print(\"Starting worker {}\\n\".format(i))\n",
        "      worker.start()\n",
        "\n",
        "    moving_average_rewards = []  # record episode reward to plot\n",
        "    while True:\n",
        "      reward = res_queue.get()\n",
        "      if reward is not None:\n",
        "        moving_average_rewards.append(reward)\n",
        "      else:\n",
        "        break\n",
        "    [w.join() for w in workers]\n",
        "    plt.plot(moving_average_rewards)\n",
        "    plt.ylabel('Moving average ep reward')\n",
        "    plt.xlabel('Step')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBOEACo3Nmq"
      },
      "source": [
        "no_of_episodes=1000\n",
        "\n",
        "Agent = A3C_Global_Network()\n",
        "Agent.train(no_of_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG_xeBu44sWI"
      },
      "source": [
        "With the help of below code we run our algorithm and see the success of it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCqBoViC4ueq"
      },
      "source": [
        "class tester:\n",
        "\n",
        "  def __init__(self,global_actor_path):\n",
        "      self.Actor_model = load_model(global_actor_path)     #import model\n",
        "      self.action_shape = 3\n",
        "      self.actions_ = np.identity(self.action_shape,dtype=bool).tolist()\n",
        "\n",
        "  def get_action(self, state):\n",
        "    '''samples the next action based on the policy probabilty distribution \n",
        "      of the actions'''\n",
        "    action_probability_distribution=(self.Actor_model.predict(state))[0]\n",
        "    action=np.argmax(action_probability_distribution)\n",
        "    return action\n",
        "    \n",
        "\n",
        "  def get_crop_and_grayscale_frame(self,frame):\n",
        "    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame=frame[40:,:]\n",
        "    frame=frame/255.\n",
        "    frame=resize(frame,(100,160))\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gF0YSk04zHi"
      },
      "source": [
        "writer = imageio.get_writer(\"test_video.mp4\", fps=20)\n",
        "env = get_game()\n",
        "env.new_episode();\n",
        "state = env.get_state().screen_buffer\n",
        "writer.append_data(state)\n",
        "done=False\n",
        "test=tester(\"Global_Actor.h5\")\n",
        "state=test.get_crop_and_grayscale_frame(state)\n",
        "stacked_frames = np.stack((state,state,state,state),axis=2)\n",
        "stacked_frames = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2]) \n",
        "state_=stacked_frames\n",
        "episode_reward = 0\n",
        "while True:\n",
        "  action=test.get_action(state_)\n",
        "  reward=env.make_action(test.actions_[action]) \n",
        "  episode_reward+=reward\n",
        "  done = env.is_episode_finished()\n",
        "  if not done:\n",
        "    next_state = env.get_state().screen_buffer\n",
        "    writer.append_data(next_state)\n",
        "    next_state=test.get_crop_and_grayscale_frame(next_state)\n",
        "    next_state_ = next_state.reshape(1,next_state.shape[0],next_state.shape[1],1)\n",
        "    stacked_frames_1 = np.append(next_state_, stacked_frames[:, :, :, :3], axis=3)\n",
        "    next_state_=stacked_frames_1\n",
        "  else:\n",
        "    next_state_=stacked_frames\n",
        "  state_=next_state_\n",
        "  if done:\n",
        "    break\n",
        "writer.close()\n",
        "print('Reward:',episode_reward)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}