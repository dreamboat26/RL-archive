{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proximal Policy Optimization (PPO).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATKIvL6uYrM0",
        "colab_type": "text"
      },
      "source": [
        "This is the Part-8 of the Deep Reinforcement Learning Notebook series. In this Notebook I have introduced Proximal Policy Optimization (PPO). \n",
        "\n",
        "The Notebook series is about Deep RL algorithms so it excludes all other techniques that can be used to learn functions in reinforcement learning and also the Notebook Series is not exhaustive i.e. it contains the most widely used Deep RL algorithms only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vCIBT3tbCAM",
        "colab_type": "text"
      },
      "source": [
        "# What is Proximal Policy Optimization (PPO)\n",
        "Proximal Policy Optimization (PPO) was intoduced by Schulman et al in the paper \"Proximal Policy Optimization Algorithms” (2017).It is a another family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzAf773ubtbN",
        "colab_type": "text"
      },
      "source": [
        "# Challenge when training agents with older policy gradient algorithms \n",
        "One challenge when training agents with policy gradient algorithms is that they are susceptible to performance collapse, in which an agent suddenly starts to perform badly. This scenario can be hard to recover from because an agent will start to generate poor trajectories, which are then used to further train the policy. Second is that on-policy algorithms are sample-inefficient because they cannot reuse data.PPO is class of optimization algorithms that addresses these two issues.\n",
        "\n",
        "The main idea behind PPO is to introduce a surrogate objective which avoids performance collapse by guaranteeing monotonic policy improvement. This objective also has the benefit of permitting off-policy data to be reused.\n",
        "PPO can be used to extend REINFORCE or Actor-Critic by replacing the original objective J($π_θ$) with the modified PPO objective. This modification leads to more stable and more sample-efficient training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWlcJp5ce3aE",
        "colab_type": "text"
      },
      "source": [
        "#Performance Collapse\n",
        "When we discussed policy gradient algorithms we learned about Policy Objective Function.We know that a policy $π_θ$ is optimized by changing the policy parameters θ using the policy gradient $▽_θJ(π_θ)$.This is an indirect approach because we are searching for the optimal policy in the policy space by making changes in parameters in parameter space. Unfortunately the policy space and the parameter space do not always map congruently, and the distances in both spaces don’t necessarily correspond.Say $θ_1, θ_2$ and $θ_2, θ_3$ and suppose they have the same distance in the parameter space, that is, $d_θ(θ_1, θ_2) = d_θ(θ_2, θ_3)$. However, their mapped policies $π_{θ_1}, π_{θ_2}$ and $π_{θ_2}, π_{θ_3}$ do not neccessarily have the same distance.\n",
        "This presents a potential problem because the ideal step size α for a parameter update becomes difficult to determine. It is not clear beforehand how small or large a step in the policy space Π it is going to map to. If α is too small, it will require many iterations and training will take a long time. Alternatively, the policy may also get stuck in some local maxima. If α is too large, the corresponding step in policy space will overshoot the neighborhood of good policies and cause a performance collapse. When this happens, the new policy that is much worse will be used to generate bad trajectory data for the next update and ruin future policy iterations. Because of this, a performance collapse can be very difficult to recover from. In general, a constant step size does not avoid these issues. \n",
        "\n",
        "\n",
        "\n",
        "The optimal choice for α will vary depending on where the current policy $π_θ$ is in the policy space Π, and how the current parameter θ maps to the neighborhood of $π_θ$ in Π from its own neighborhood in Θ. Ideally an algorithm should adaptively vary the parameter step size based on these factors.\n",
        "To derive an adaptive step size based on how a particular update in parameter space will affect policy space, we first need way to measure the difference in performance between two policies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZvYHZj-hUMG",
        "colab_type": "text"
      },
      "source": [
        "# Relative Policy Performance Identity And Surrogate Objective\n",
        "We can define a relative policy performance identity as the difference in the objectives between them, shown in Equation 7.1.\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%204.29.31%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.1\n",
        "\n",
        "The relative policy performance identity J(π′) – J(π) serves as a metric used to measure policy improvements. If the difference is positive, the newer policy π′ is better than π. During policy iteration, we should ideally choose a new policy π′ such that this difference is maximized. Therefore, maximizing objective J(π′) is equivalent to maximizing this identity, and they can both be done by gradient ascent.\n",
        "\n",
        "Framing the objective this way also means that every policy iteration should ensure a nonnegative (monotonic) improvement, i.e. J(π′) – J(π)≥ 0, since in the worst case we can simply let π′ = π to obtain no improvement. With this, there will be no performance collapses throughout the training, which is the property we are looking for.\n",
        "\n",
        "However, there is a limitation that prohibits the identity from being used as an objective function. Note that in its expression $E_{τ∼π′}[ ∑_{t≥0} A^π(s_t, a_t)]$, the expectation requires trajectories to be sampled from the new policy π′ for an update, but π′ is not available until after the update. To remedy this paradox, we need to find a way to alter it such that it uses the old policy π that is available.\n",
        "To this end, we can assume that successive policies π, π′ are relatively close (this can be measured by low KL divergence), such that the state distributions from both are similar.\n",
        "\n",
        "Then, Equation 7.1 can be approximated by using trajectories from the old policy, τ ∼ π, adjusted with importance sampling weights $π′(a_t/s_t)$/$π(a_t/s_t)$. This adjusts the returns generated using π by the ratio of action probabilities between the two successive policies π, π . The rewards associated with actions that are more likely under the new policy π′ will be up-weighted, and rewards associated with relatively less actions under π′ will be down- weighted. This approximation is shown in Equation 7.2.\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%204.42.26%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.2\n",
        "\n",
        "The new objective on the right hand side of Equation 7.2 $J_π^{CPI}(π')$ , is called a surrogate objective because it contains\n",
        "a ratio of the new and old policies, π′ and π. The superscript CPI stands for “conservative policy iteration”.\n",
        "\n",
        "The proof is skipped but it can be show that the gradient of the surrogate objective is equal to the policy gradient, as stated in Equation 7.3.\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%204.49.10%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PioElEM1nKsh",
        "colab_type": "text"
      },
      "source": [
        "Before $J_π^{CPI}(π')$ can stand in as the new objective for a policy gradient algorithm, there is one final requirement to satisfy.\n",
        "$J_π^{CPI}(π')$ is only an approximation for J(π′) – J(π), so it will contain some error. However, we want to guarantee when we\n",
        "use $J_π^{CPI}(π')$ ~ J(π′) – J(π) that J(π′) – J(π) ≥ 0. We therefore need to understand the error introduced by the\n",
        "approximation.We express the absolute error by taking the difference between the new objective J(π′) and its estimated improvement J(π)+ $J_π^{CPI}(π')$.Then, the error can be bounded in terms of the KL divergence between π and π.\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%205.00.16%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.3\n",
        "\n",
        "C is a constant that needs to be chosen, and KL is the KL divergence.\n",
        "\n",
        "Using this, it is quite straightforward to derive the result we are after, namely \n",
        "\n",
        "J(π′) – J(π) ≥ 0. This is called the monotonic improvement theory.\n",
        "\n",
        "If we add the error bound as a penalty in our optimization problem, then we can guarantee monotonic policy improvement. Our optimization problem now becomes the following.\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%205.06.17%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.4\n",
        "\n",
        "This result satisfies our final requirement. It allows us to avoid performance collapse that may occur when using the original objective J(π). One key distinction to note is that a monotonic improvement does not guarantee convergence to an optimal policy π*. For instance, policy optimization can still get stuck at a local maxima in which every policy iteration produces no improvement, i.e. J(π′) – J(π) = 0. Guaranteeing convergence remains a difficult open problem.\n",
        "The final step step is to consider how to implement the optimization problem from Equation 7.4 in practice. One idea is to directly constrain the expectation of KL as shown in Equation 7.5.\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%205.08.45%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.5\n",
        "\n",
        "δ limits how large KL divergence can be, so it effectively restricts how far a new policy π′ can diverge away from the old policy π. Only candidates in a close neighborhood around π in the policy space will be considered. This neighborhood is called a trust region, and Equation 7.5 is called a trust region constraint. Note that δ is a hyperparameter that needs to be tuned.\n",
        "\n",
        "\n",
        "Putting the constraint and surrogate objective together, the trust region policy optimization problem is shown in Equation 7.6.\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%206.16.07%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.6\n",
        "\n",
        "To sum up, $J_π^{CPI}(π')$ is a linear approximation to J(π′) – J(π)\n",
        "since its gradient is equal to the policy gradient. It also guarantees monotonic improvement to within an error bound. To ensure improvement accounting for this potential error, we impose a trust region constraint to limit the difference between the new and the old policies. Provided changes in the policy stay within the trust region, we can avoid performance collapse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOXg5fbq6uks",
        "colab_type": "text"
      },
      "source": [
        "# PROXIMAL POLICY OPTIMIZATION (PPO)\n",
        "\n",
        "PPO is easy to implement, computationally inexpensive, and we do not have to choose δ. For these reasons, it has become one of the most popular policy gradient algorithms.\n",
        "PPO is a family of algorithms that solves the trust-region constrained policy optimization problem with simpler and effective heuristics. Two variants exist. The first is based on an adaptive KL penalty, and the second is based on a clipped objective.\n",
        "\n",
        "The first variant of PPO is called PPO with adaptive KL penalty, with the objective shown in Equation 7.7. It turns the KL constraint $E_t[KL(π_θ(a_t|s_t)||π_{old}(a_t|s_t))] ≥ δ $ into an adaptive KL penalty which is subtracted from the importance-weighted advantage. The expectation of the result is the new objective to be maximized.\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%206.21.21%20PM.png?raw=true)\n",
        "\n",
        "\n",
        "Equation 7.7\n",
        "\n",
        "Equation 7.7 is known as a KL-penalized surrogate objective. β is an adaptive coefficient which controls the size of the KL penalty. It serves the same purpose as Equation 7.32 in controlling the trust region of optimization. The larger β the more the difference between $π_θ$ and $π_{θold}$. The smaller β the higher the tolerance between the two policies.\n",
        "\n",
        "One challenge of using a constant coefficient is that different problems have different characteristics, so it is hard to find one which will work for all of them. Even on a single problem, the loss landscape changes as policy iterates, so a β that works earlier may not work later. β needs to adapt accordingly to these changes.\n",
        "To tackle this problem, the PPO paper proposes a heuristic- based update rule for β that allows it to adapt over time. β is updated after every policy update and the new value is used in the next iteration. The update rule for β is shown below. It can be used as a subroutine in the PPO algorithm which will be presented later.\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%206.27.24%20PM.png?raw=true)\n",
        "\n",
        "\n",
        "At the end of each iteration, we estimate δ and compare it to the desired target $δ_{tar}$. If δ is less than $δ_{tar}$ by some margin, we decrease the KL penalty by reducing β. However, if δ is greater than $δ_{tar}$ by some margin, we increase the KL penalty by increasing β. The specific values used to determine the margins and the update rules for β are chosen empirically. The authors selected 1.5 and 2 respectively, but also found that the algorithm is not very sensitive to these values.\n",
        "It was also observed that KL divergence occasionally departs far away from from its target value $δ_{tar}$, but β adjusts quickly. Good values for $δ_{tar}$ also needs to be determined empirically.\n",
        "This approach has the benefit of being simple to implement. However, it hasn’t overcome the problem of needing to choose a target δ. Furthermore, it can be computationally expensive because of the need to calculate the KL.\n",
        "\n",
        "PPO with clipped surrogate objective remedies this by doing away with the KL constraint and opts for a simpler modification to the surrogate objective as shown in Equation 7.8.\n",
        "\n",
        "\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%206.30.42%20PM.png?raw=true)\n",
        "\n",
        "Equation 7.8\n",
        "\n",
        "Equation 7.8 is known as clipped surrogate objective. ∊ is a\n",
        "value which defines the clipping neighborhood |$r_t$(θ) – 1| ≤ ∊. It\n",
        "is a hyperparameter to be tuned, and can be decayed during\n",
        "training. The first term within min(·) is simply the surrogate $J^{CPI}$.The second term clip ($r_t$(θ), 1 – ∊, 1 + ∊)At bounds the value of $J^{CPI}$ to be between (1 – ∊)$A_t$ and (1 + ∊)$A_t$. When $r_t$(θ) is within the interval [1 – ∊, 1 + ∊], both terms inside min(·) are equal. This objective prevents parameter updates which could cause large and risky changes to the policy πθ. For the purpose of quantifying large policy changes, the probability ratio rt(θ) is useful. \n",
        "\n",
        "The clipped objective $J^{CLIP}$ is cheap to compute, very easy to understand, and implementation requires only a few trivial modifications to the original surrogate objective $J^{CPI}$.\n",
        "The most costly computations are the probability ratio rt(θ) and advantage At. However, these are the minimum calculations needed by any algorithms optimizing the surrogate objective. The other calculations are clipping and minimizing, which are essentially constant-time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsWzBZbPAJNf",
        "colab_type": "text"
      },
      "source": [
        "# PPO with clipping, extending Actor- Critic Algorithm\n",
        "![alt text](https://github.com/Machine-Learning-rc/Unimportant/blob/master/Screenshot%202020-07-31%20at%206.46.01%20PM.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYta_09hBoWJ",
        "colab_type": "text"
      },
      "source": [
        "# IMPLEMENTING PPO with clipping, extending Actor- Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgILhHtfEbzp",
        "colab_type": "text"
      },
      "source": [
        "Below code setups the environment required to run and record the game and also loads the required library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTCiTOajEbgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRPoVx0KEuYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input\n",
        "from tensorflow.keras.models import Sequential,load_model,Model\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.utils import normalize as normal_values\n",
        "import cv2\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython.display import clear_output\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4hiXbC3Eu3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQpd6qK0EwXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VsAqLw6E0FH",
        "colab_type": "text"
      },
      "source": [
        "This part ensures the reproducibility of the code below by using a random seed and setups the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZWrySw7Exoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED=3\n",
        "# random seed (reproduciblity)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# set the env\n",
        "env = (gym.make(\"Ant-v2\"))  # env to import\n",
        "#env=wrap_env(env)  #use this when you want to record a video of episodes\n",
        "\n",
        "env.seed(RANDOM_SEED)\n",
        "env.reset() # reset to env\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBeYKmXHGALH",
        "colab_type": "text"
      },
      "source": [
        "Defining the PPO Class. At initiation, the PPO object sets a few parameters like environment, action and state space,create Actor,Critic,and target Actor models,and remember(that records the observations of each step.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NP5V_8YGWRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PPO:\n",
        " \n",
        "  def __init__(self,Actor_path=None,Critic_path=None,old_Actor_path=None):\n",
        "    #self.env=env #import env\n",
        "    self.state_shape=111 # the state space\n",
        "    self.action_shape=env.action_space.shape[0] # the action space\n",
        "    self.upper_bound=upper_bound\n",
        "    self.lower_bound=lower_bound\n",
        "    self.gamma=[0.99] # decay rate of past observations\n",
        "    self.learning_rate=3e-5 # learning rate in deep learning\n",
        "    self.lambda_=0.95\n",
        "    self.epochs=7\n",
        "    self.tau=0.005\n",
        "    self.batch_size=64\n",
        "    self.beta=0.001 #Entropy Loss ratio\n",
        "    self.clipping_ratio = 0.2\n",
        "    if not Actor_path:\n",
        "      self.Actor_model=self._create_model('Actor')    #Target Model is model used to calculate target values\n",
        "      self.target_Actor_model=self._create_model('Actor')\n",
        "      self.Critic_model=self._create_model('Critic')  #Training Model is model to predict q-values to be used.\n",
        "    else:\n",
        "      self.Actor_model=load_model(Actor_path) #import model\n",
        "      self.Critic_model=load_model(Critic_path) #import model\n",
        "      self.target_Actor_model=load_model(old_Actor_path)\n",
        "    \n",
        "        # record observations\n",
        "    self.states=deque()\n",
        "    self.rewards=deque()\n",
        "    self.actions=deque()\n",
        "    self.last_state=np.zeros((1,self.state_shape))\n",
        "  \n",
        "  def remember(self, state,reward,action,last_state,done):\n",
        "    '''stores observations'''\n",
        "    self.states.append(state)\n",
        "    self.actions.append(action)\n",
        "    self.rewards.append(reward)\n",
        "    if done:\n",
        "      self.last_state[0]=last_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_7HXuE1HII9",
        "colab_type": "text"
      },
      "source": [
        "Creating a Neural Network Model (Actor and Critic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yz4yFq4HLE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_model(self,model_type):\n",
        " \n",
        "    ''' builds the model using keras'''\n",
        " \n",
        "    state_input=Input(shape=(self.state_shape,))\n",
        "    layer_1=Dense(128,activation='relu')(state_input)\n",
        "    layer_2=Dense(128,activation='relu')(layer_1)\n",
        "    \n",
        " \n",
        "    if model_type=='Actor':\n",
        "      output =(Dense(2*self.action_shape, activation=None))(layer_2)\n",
        "      model = Model(inputs=[state_input],outputs=[output])\n",
        "    else:\n",
        "      output=Dense(1, activation=None,kernel_initializer=tf.keras.initializers.random_uniform(minval=-0.0003,maxval=0.0003))(layer_2)\n",
        "      model = Model(inputs=[state_input],outputs=[output])\n",
        "      model.compile(optimizer=Adam(learning_rate=self.learning_rate),loss=\"mse\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgrGYw-5HOW3",
        "colab_type": "text"
      },
      "source": [
        "Action Selection  and Probability Calcualtion\n",
        "\n",
        "The get_action method guides the action choice. We get mean and standard deviation from Actor Model and then we use them to get action. Then we clip the action so that action doesn't got out of legal actions range.\n",
        "\n",
        "\n",
        "The log_policy_prob method provides us with the probability of the action This method is used for caculating probs later while training the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYBNlT5NHSBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def get_action(self, state,status=\"Training\"):\n",
        "    action_mean,action_std=self.Actor_model(state)\n",
        "    action = action_mean.numpy() + np.exp(action_std.numpy()) * np.random.normal(size=action_std.shape)\n",
        "    action=action*self.upper_bound\n",
        "    legal_action = np.clip(action, self.lower_bound, self.upper_bound)\n",
        "    return legal_action[0]\n",
        "\n",
        "  def log_policy_prob(self,mean, std, action):\n",
        "    # policy log probability\n",
        "    exp_coeff= -(((action-mean)/(np.exp(std)))**2)/2\n",
        "    act_log_softmax = tf.math.exp(exp_coeff)/(tf.math.sqrt(2*math.pi)*(np.exp(std)))\n",
        "    return tf.reduce_sum(act_log_softmax,axis=1,keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmT9fn5PHy4L",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The get_GAEs method calculates Generalized advantage estimation (GAE) which later is used to calculate Advantage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wKLAORDIPeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def get_GAEs(self,v_preds,rewards):\n",
        "    T = len(v_preds)-1\n",
        "    gaes = np.zeros((T,1),dtype='float32')\n",
        "    future_gae = 0.0\n",
        "    for t in reversed(range(T)):\n",
        "      delta = rewards[t] + np.asarray(self.gamma) * v_preds[t + 1] - v_preds[t]\n",
        "      gaes[t] = future_gae = delta + np.asarray(self.gamma) * np.asarray(self.lambda_) *np.asarray(future_gae)\n",
        "    return gaes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyWKswLHId0_",
        "colab_type": "text"
      },
      "source": [
        "Updating networks\n",
        "\n",
        "We update critic and actor according to loss that we discussed earlier and do soft updates on target network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H815Q2OfIg--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def update_models(self):\n",
        "\n",
        "    states_mb=np.zeros((self.batch_size,self.state_shape))\n",
        "    V_s_mb=np.zeros((self.batch_size,1))\n",
        "    actions_mb=np.zeros((self.batch_size,self.action_shape))\n",
        "    rewards_mb=np.zeros((self.batch_size,1))\n",
        "\n",
        "    batch_indices = np.random.choice(len(self.states),self.batch_size)\n",
        "\n",
        "    for i,j in enumerate(batch_indices):\n",
        "      \n",
        "      state_=self.states[j]\n",
        "      states_mb[i]=state_\n",
        "      actions_mb[i]=(self.actions[j])\n",
        "      rewards_mb[i]=self.rewards[j]\n",
        "\n",
        "      \n",
        "      vs_=self.Critic_model.predict(state_)\n",
        "      V_s_mb[i]=vs_\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    V_last_state=self.Critic_model.predict(self.last_state)\n",
        "    v_all=np.concatenate((V_s_mb,V_last_state),axis=0)\n",
        "    \n",
        "    Advantages=self.get_GAEs(v_all,rewards_mb)\n",
        "    critic_targets = Advantages +  V_s_mb\n",
        "    self.Critic_model.fit(states_mb, critic_targets,epochs=self.epochs,verbose=2)\n",
        "    \n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "    def train_step(states_1_mb,Advantages,actions):\n",
        "      with tf.GradientTape() as tape:\n",
        "        Advantages=tf.stop_gradient(Advantages)\n",
        "        mean_and_log_std=self.Actor_model(states_1_mb,training=True)\n",
        "        mean,std = tf.split(mean_and_log_std, num_or_size_splits=2, axis=1)\n",
        "        probs=self.log_policy_prob(mean,std,actions)\n",
        "        \n",
        "        old_mean_and_log_std = (self.old_Actor_model)(states_1_mb,training=True)\n",
        "        old_mean,old_std = tf.split(old_mean_and_log_std, num_or_size_splits=2, axis=1)\n",
        "        old_probs=self.log_policy_prob(old_mean,old_std,actions)\n",
        "        old_probs=tf.stop_gradient(old_probs)\n",
        "        \n",
        "        ratio =tf.math.exp(probs-old_probs)\n",
        "        clip_ratio = tf.clip_by_value(ratio, clip_value_min=1 - self.clipping_ratio, clip_value_max=1 + self.clipping_ratio)\n",
        "        surrogate=(tf.math.minimum(ratio*Advantages,clip_ratio*Advantages))\n",
        "        entropy_loss=-(probs * tf.math.log(probs+1e-10))\n",
        "        ppo_loss = -(tf.math.reduce_mean(surrogate+self.beta* entropy_loss))                      \n",
        "      grads = tape.gradient(ppo_loss,self.Actor_model.trainable_variables)\n",
        "      grads = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in grads]\n",
        "      optimizer.apply_gradients(zip(grads, self.Actor_model.trainable_variables))\n",
        "      return ppo_loss\n",
        "\n",
        "    ppo_loss=[]\n",
        "    for epoch in range(self.epochs):\n",
        "      loss=train_step(states_mb,Advantages,actions_mb)\n",
        "      ppo_loss.append(loss)\n",
        "    print(\"PPO_loss:\",np.mean(ppo_loss)) \n",
        "   \n",
        "    actor_weights = np.array(self.Actor_model.get_weights())\n",
        "    actor_target_weights = np.array(self.old_Actor_model.get_weights())\n",
        "    new_weights = self.tau*actor_weights + (1-self.tau)*actor_target_weights\n",
        "    self.old_Actor_model.set_weights(new_weights)\n",
        "\n",
        "    self.states.clear();self.rewards.clear();self.actions.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch8NrPpFJY8i",
        "colab_type": "text"
      },
      "source": [
        "Training the model\n",
        "\n",
        "This method creates a training environment for the model. Iterating through a set number of episodes, it uses the model to sample actions and play them. When such a sequence ends, the model is using the recorded observations to update the policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZvVHkCXJZTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def train(self,episodes):\n",
        "    env = (gym.make(\"Ant-v2\"))\n",
        "    done_1=False\n",
        "    for episode in range(episodes):\n",
        "      T_max=1000\n",
        "      state=env.reset()\n",
        "      done=False\n",
        "      state_=state.reshape((1,111))\n",
        "      episode_reward=0  \n",
        "      print(\"Episode Started\")\n",
        "      for t in range(T_max):\n",
        "        action=self.get_action(state_)\n",
        "        next_state, reward, done, info=env.step(action)\n",
        "        reward_=reward\n",
        "        next_state_=next_state.reshape((1,111))\n",
        "        if t==T_max-1 or done:\n",
        "          done_1=True\n",
        "        self.remember(state_,reward,action,next_state_,done_1)\n",
        "        state_ = next_state_\n",
        "        episode_reward+=reward\n",
        "        if done:\n",
        "          break\n",
        "      self.Average_rewards.append(episode_reward)\n",
        "      avg_reward = np.mean(self.Average_rewards[-100:])\n",
        "      print('Episode Ended')\n",
        "      print(\"Episode:{}  Reward:{} Average_reward:{}\".format(episode,episode_reward,avg_reward))\n",
        "      print(\"Updating the model\")\n",
        "      self.update_models()\n",
        "      done_1=False\n",
        "      if episode%100==0 and episode!=0:\n",
        "        self.Actor_model.save('Ant-v2_Actor_{}.h5'.format(episode))\n",
        "        self.old_Actor_model.save('Ant-v2_old_Actor_{}.h5'.format(episode))\n",
        "        self.Critic_model.save('Ant-v2_Critic_{}.h5'.format(episode))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzcgtHDD3Gtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Agent_2=PPO()\n",
        "Agent_2.train(10000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}