{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Reinforcement_Learning_Part_12_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrWT7652JGFI"
      },
      "source": [
        "This is the Part-12 of the Deep Reinforcement Learning Notebook series. In this Notebook I have introduced Rainbow Algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6roxCNJJKX0"
      },
      "source": [
        "\n",
        "The Notebook series is about Deep RL algorithms so it excludes all other techniques that can be used to learn functions in reinforcement learning and also the Notebook Series is not exhaustive i.e. it contains the most widely used Deep RL algorithms only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-P4isz3JhCW"
      },
      "source": [
        "##What is Rainbow Algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LAQht6uJl38"
      },
      "source": [
        "Rainbow is a DQN based off-policy deep reinforcement learning algorithm with six extensions of DQN's that each have addressed a limitation and improved overall performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9vw0gp9GG8O"
      },
      "source": [
        "##Extensions Used\n",
        "\n",
        "1) Double Q-learning\n",
        "\n",
        "=> Double Q-learning address problem of overestimation of Q-values by neural network.\n",
        "\n",
        "2) Prioritized replay\n",
        "\n",
        "=> Prioritized replay replaces the randomly sampling process in DQN by efficiently sampling the transitions from which there is much to learn.\n",
        "\n",
        "3) Dueling networks\n",
        "\n",
        " => The dueling network is a neural network architecture designed for value based RL. It features two streams of computation, the value and advantage streams, sharing a convolutional encoder, and merged by a special aggregator\n",
        "\n",
        "You can read more about Double Q-learning, Prioritized replay, Dueling networks at https://github.com/Rahul-Choudhary-3614/Deep-Reinforcement-Learning-Notebooks/blob/master/Deep_Reinforcement_Learning_Part_5_.ipynb\n",
        "\n",
        "4) Multi-step learning\n",
        "\n",
        "=> When calculating the target value in Q-Learning, the target value is based on only the current reward. For N-step Q-Learning, rewards from N steps are added together and the Q function value is added only at the very end.\n",
        "\n",
        "\n",
        "5) Distributional RL\n",
        "\n",
        "=> We can learn to approximate the distribution of returns instead of the expected return.\n",
        "\n",
        "6) Noisy Nets\n",
        "\n",
        "=> Noisy Nets are way to improve exploration in the environment by adding noise to network parameters\n",
        "\n",
        "You can read more about Noisy networks at https://github.com/Rahul-Choudhary-3614/Deep-Reinforcement-Learning-Notebooks/blob/master/Deep_Reinforcement_Learning_Part_11_.ipynb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Full Rainbow paper => [https://arxiv.org/pdf/1710.02298.pdf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3tZRkrroXRr"
      },
      "source": [
        "Below code loads the required library and 2 components for our algorithm i.e. Priority Experience Replay memory buffer and Noisy Dense Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwTDTPXzldKO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Dropout,Conv2D,Flatten,MaxPooling2D,Activation\n",
        "import numpy as np\n",
        "import gym\n",
        "import pickle\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "from collections import deque\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "%matplotlib inline\n",
        "from PriorityExperienceReplay import Memory\n",
        "from noisy_nets import noisy_dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdgqNweooU4X"
      },
      "source": [
        "This part ensures the reproducibility of the code below by using a random seed and setups the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwtjJq6Lld-5"
      },
      "source": [
        "RANDOM_SEED=1\n",
        "\n",
        "# random seed (reproduciblity)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# set the env\n",
        "env_name = \"Bowling-v0\" \n",
        "env = gym.make(env_name)\n",
        "env.seed(RANDOM_SEED)\n",
        "env.reset();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0wEhmFvpDwW"
      },
      "source": [
        "This parts initializes parameter necessary to implement Distributional RL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9Lk3ilapUr6"
      },
      "source": [
        "N_atoms = 51\n",
        "V_Max = 20.0\n",
        "V_Min = 0.0\n",
        "Delta_z = (V_Max - V_Min)/(N_atoms - 1)\n",
        "z_list = tf.constant([V_Min + i * Delta_z for i in range(N_atoms)],dtype=tf.float32)\n",
        "z_list_broadcasted = tf.tile(tf.reshape(z_list,[1,N_atoms]), tf.constant([action_shape,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAhcQuIPpY7f"
      },
      "source": [
        "This parts initializes necessary hyper-parameters for our algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uag9enVVlhRE"
      },
      "source": [
        "observing_episodes = 4    #No of observations before updating the training network\n",
        "observing_episodes_target_model = 10000 #No of observations before updating the target network\n",
        "learning_rate = 3e-3 # learning rate \n",
        "\n",
        "epsilon_initial_value = 1.0 # initial value of epsilon\n",
        "epsilon_current_value = 1.0# current value of epsilon\n",
        "epsilon_final_value = 0.001 # final value of epsilon\n",
        "\n",
        "batch_size = 256\n",
        "gamma = 0.99 # decay rate of past observations\n",
        "state_shape = (76, 160, 4) # the state space\n",
        "action_shape = env.action_space.n # the action space\n",
        "memory_size = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBL8fzaupnF1"
      },
      "source": [
        "Creating a Rainbow Algorithm Model Class. Here we have used 2 noisy dense layers as final layers of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksguratzloGE"
      },
      "source": [
        "class _rainbow_model(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self,state_shape,action_shape,N_atoms):\n",
        "    super(_rainbow_model,self).__init__()\n",
        "\n",
        "    self.layer_1 = Conv2D(32,5,strides=2,input_shape=(state_shape))\n",
        "    self.layer_2 = MaxPooling2D(pool_size=(2,2))\n",
        "    self.layer_3 = Activation('relu')\n",
        "\n",
        "    self.layer_4 = Conv2D(64,3)\n",
        "    self.layer_5 = MaxPooling2D(pool_size=(2,2))\n",
        "    self.layer_6 = Activation('relu')\n",
        "\n",
        "    self.layer_7 = Conv2D(128,3)\n",
        "    self.layer_8 = MaxPooling2D(pool_size=(2,2))\n",
        "    self.layer_9 = Activation('relu')\n",
        "\n",
        "    self.layer_10 = Flatten()\n",
        "    self.layer_11 = noisy_dense(64)\n",
        "    self.layer_12 = Activation('relu')\n",
        "\n",
        "    self.layer_13 = noisy_dense(action_shape*N_atoms)\n",
        "  \n",
        "  def call(self,x):\n",
        "    x = self.layer_3(self.layer_2(self.layer_1(x)))\n",
        "    x = self.layer_6(self.layer_5(self.layer_4(x)))\n",
        "    x = self.layer_9(self.layer_8(self.layer_7(x)))\n",
        "    x = self.layer_12(self.layer_11(self.layer_10(x)))\n",
        "    x = self.layer_13(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaNtncrtp_T3"
      },
      "source": [
        "Defining the Rainbow Class. You can see that I have commented out few things like temperature_parameter .You can uncomment them if you can want to use Boltzmann policy.In this epsilon greedy works good. So I have used that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDWR4EiNmDLA"
      },
      "source": [
        "class Rainbow():\n",
        "\n",
        "  def __init__(self,env,memory_size,path_1=None,path_2=None):\n",
        "    self.env = env\n",
        "    self.memory = Memory(memory_size)\n",
        "    self.learning_rate = learning_rate\n",
        "    self.state_shape = state_shape\n",
        "    self.action_shape = action_shape\n",
        "    self.epsilon_current_value = epsilon_current_value\n",
        "    self.epsilon_initial_value = epsilon_initial_value\n",
        "    self.epsilon_final_value = epsilon_final_value\n",
        "    self.observing_episodes = 10  \n",
        "    self.observing_episodes_target_model = 200\n",
        "    #self.temperature_parameter_initial_value=5.0 # initial value of epsilon\n",
        "    #self.temperature_parameter_current_value=5.0# current value of epsilon\n",
        "    #self.temperature_parameter_final_value=1.0 # final value of epsilon\n",
        "    self.gamma  = gamma\n",
        "    self.batch_size = batch_size\n",
        "    self._num_step = 2000\n",
        "\n",
        "    if not path_1:\n",
        "      self.target_model =  _rainbow_model(self.state_shape,self.action_shape,N_atoms)    #Target Model is model used to calculate target values\n",
        "      self.training_model = _rainbow_model(self.state_shape,self.action_shape,N_atoms)  #Training Model is model to predict q-values to be used.\n",
        "    else:\n",
        "      self.training_model=load_model(path_1)\n",
        "      self.target_model=load_model(path_2)\n",
        "\n",
        "  def get_frame(self,frame):\n",
        "    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame=frame[100:-34,:]\n",
        "    frame=frame/255.\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epsYKbPWqSpp"
      },
      "source": [
        "The model output is first passed through  softmax layer and clipped to get distributional q values which are some algebraic manipulation are converted to q values.\n",
        "\n",
        "This is the Distributional RL part of the Rainbow algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vri5QwwPmHS0"
      },
      "source": [
        "  def get_q_values(self,model_output):\n",
        "    q_distributional = tf.reshape(model_output, [-1, self.action_shape, N_atoms])\n",
        "    q_distributional = tf.nn.softmax(q_distributional, axis = 2)\n",
        "    q_distributional = tf.clip_by_value(q_distributional, 1e-8, 1.0-1e-8)\n",
        "\n",
        "    q_values =  tf.multiply(q_distributional, z_list_broadcasted)\n",
        "    q_values = tf.reduce_sum(q_values, axis=2)\n",
        "    return q_distributional,q_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqIo-kQkqP2O"
      },
      "source": [
        "Action Selection \n",
        "\n",
        "The get_action method guides out action choice. Initially, when training begins we use exploration policy but later we do exploitation.\n",
        "\n",
        "You can uncomment the commented lines to use Boltzmann exploration policy instead of epsilon greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t2gqshamJId"
      },
      "source": [
        "  def get_action(self, state,status='Training'):\n",
        "    '''samples the next action based on the E-greedy policy'''\n",
        "\n",
        "    if status==\"Evaluating\":\n",
        "       _ , q_values = self.get_q_values(self.training_model(state))\n",
        "       action = np.argmax(q_values)\n",
        "       return action\n",
        "\n",
        "    if random.random() < self.epsilon_current_value:                                    #Exlporation\n",
        "      #_ , q_values = self.get_q_values(self.training_model.predict(state))\n",
        "      #q_values=(q_values[0])**(1/self.temperature_parameter_current_value)  #This is the step where we use Boltzmann exploration policy\n",
        "      #top_actions=q_values.argsort()[-self.nma:][::-1]\n",
        "      #action=random.choice(top_actions)\n",
        "      action = random.choice(list(range(self.action_shape)))\n",
        "    else:\n",
        "      _ , q_values = self.get_q_values(self.training_model(state)) #Exploitation\n",
        "      action = np.argmax(q_values)\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy6WYLXqrL8M"
      },
      "source": [
        "This function is used to get the overall loss of the alorithm. To fully understand this you are suggested to read the paper ([https://arxiv.org/pdf/1710.02298.pdf])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_clw-rDrLt3"
      },
      "source": [
        "  def get_loss(self,states_mb,dones_mb,rewards_mb,preds_mb,actions_mb,ISWeights_mb):\n",
        "    \n",
        "    states_mb = tf.cast(states_mb,tf.float32)\n",
        "    dones_mb = tf.cast(dones_mb,tf.float32)\n",
        "    rewards_mb = tf.cast(rewards_mb,tf.float32)\n",
        "    preds_mb = tf.cast(preds_mb,tf.float32)\n",
        "    actions_mb = tf.cast(actions_mb,tf.float32)\n",
        "    ISWeights_mb = tf.cast(ISWeights_mb,tf.float32)\n",
        "\n",
        "    Q_distributional_values_target,_ = self.get_q_values((self.target_model(states_mb)))\n",
        "    Q_distributional_values_target = tf.cast(Q_distributional_values_target,tf.float32)\n",
        "    tmp_batch_size = tf.shape(Q_distributional_values_target)[0]\n",
        "    tmp_batch_size = tf.cast(tmp_batch_size,tf.float32)\n",
        "    preds_mb = tf.convert_to_tensor(np.asarray(np.array(list(enumerate(preds_mb)),dtype=object).astype('int32')))\n",
        "\n",
        "    Q_distributional_chosen_by_action_target = tf.gather_nd(Q_distributional_values_target,preds_mb)\n",
        "\n",
        "    target = tf.tile(tf.reshape(rewards_mb,[-1, 1]), tf.constant([1, N_atoms])) + (self.gamma**self._num_step) * tf.multiply(tf.reshape(z_list,[1,N_atoms]),(1.0 - tf.tile(tf.reshape(dones_mb ,[-1, 1]), tf.constant([1, N_atoms]))))\n",
        "    target = tf.cast(target,tf.float32)\n",
        "    target = tf.clip_by_value(target, V_Min, V_Max)\n",
        "    b = (target - V_Min) / Delta_z\n",
        "    u, l = tf.math.ceil(b), tf.math.floor(b)\n",
        "    u_id, l_id = tf.cast(u, tf.int32), tf.cast(l, tf.int32)\n",
        "    u_minus_b, b_minus_l = u - b, b - l\n",
        "\n",
        "    Q_distributional_values_online,_ = self.get_q_values((self.training_model(states_mb))) \n",
        "    Q_distributional_values_online = tf.cast(Q_distributional_values_online,tf.float32)\n",
        "    actions_mb = tf.convert_to_tensor(np.asarray(np.array(list(enumerate(actions_mb)),dtype=object).astype('int32')))\n",
        "    Q_distributional_chosen_by_action_online = tf.gather_nd(Q_distributional_values_online, actions_mb)\n",
        "\n",
        "    index_help = tf.tile(tf.reshape(tf.range(tmp_batch_size),[-1, 1]), tf.constant([1, N_atoms])) \n",
        "    index_help = tf.expand_dims(index_help, -1)\n",
        "\n",
        "    index_help = tf.cast(index_help,tf.int32)\n",
        "    u_id = tf.cast(u_id,tf.int32)\n",
        "    l_id = tf.cast(l_id,tf.int32)\n",
        "\n",
        "    u_id = tf.concat([index_help, tf.expand_dims(u_id, -1)], axis=2)\n",
        "    l_id = tf.concat([index_help, tf.expand_dims(l_id, -1)], axis=2)\n",
        "    error = Q_distributional_chosen_by_action_target * u_minus_b * tf.math.log(tf.gather_nd(Q_distributional_chosen_by_action_online, l_id)) + Q_distributional_chosen_by_action_target * b_minus_l * tf.math.log(tf.gather_nd(Q_distributional_chosen_by_action_online, u_id))\n",
        "    error = tf.reduce_sum(error, axis=1)\n",
        "    loss = tf.negative(error * ISWeights_mb)\n",
        "    error_op = tf.abs(error)\n",
        "    return error_op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-2TUvs3q_cN"
      },
      "source": [
        "Updating the model\n",
        "\n",
        "The update_training_model method updates the training model weights.\n",
        "\n",
        "The update_target_model method updates the target model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEwaOxyamLeW"
      },
      "source": [
        "  def update_training_model(self):\n",
        "    \n",
        "    tree_idx, batch, ISWeights_mb = self.memory.sample(self.batch_size)\n",
        "    \n",
        "    states_mb = np.zeros((self.batch_size,*self.state_shape))\n",
        "    dones_mb =   np.zeros((self.batch_size,1))\n",
        "    rewards_mb = np.zeros((self.batch_size,1))\n",
        "    preds_mb   = np.zeros((self.batch_size,1))\n",
        "    actions_mb = np.zeros((self.batch_size,1))\n",
        "\n",
        "    for i in range(self.batch_size):\n",
        "      states_mb[i]           =         batch[i][0][0]\n",
        "      _ ,q_values            = self.get_q_values(self.training_model(batch[i][0][0]))\n",
        "      preds_mb[i]  = np.argmax(q_values)\n",
        "      actions_mb[i] = batch[i][0][1]\n",
        "      rewards_mb[i] = (batch[i][0][2])\n",
        "      dones_mb[i] = batch[i][0][3]      \n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "    def train_step(states_mb,dones_mb,rewards_mb,preds_mb,actions_mb,ISWeights_mb):\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss = self.get_loss(states_mb,dones_mb,rewards_mb,preds_mb,actions_mb,ISWeights_mb)                \n",
        "      grads = tape.gradient(loss,self.training_model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, self.training_model.trainable_variables))\n",
        "      return loss\n",
        "\n",
        "    abs_error = train_step(states_mb,dones_mb,rewards_mb,preds_mb,actions_mb,ISWeights_mb)\n",
        "\n",
        "    # Update priority\n",
        "    abs_error=abs_error/(np.max(abs_error)+1e-30)\n",
        "    self.memory.batch_update(tree_idx, abs_error)\n",
        "\n",
        "  def update_target_model(self):\n",
        "    self.target_model.set_weights(self.training_model.get_weights()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP3XlUOUsHsM"
      },
      "source": [
        "This function is used to evaluate the algorithm during training of the algorithm and record the results in video format\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdRBbpRPmXlp"
      },
      "source": [
        "  def evaluate(self,ep,no_of_testing_episodes=20):\n",
        "    Average_Reward=[]\n",
        "    for episode in range(no_of_testing_episodes):\n",
        "      writer = imageio.get_writer(\"Evaluating_video_{}_{}.mp4\".format(ep,episode), fps=20)\n",
        "      env = (gym.make(\"Bowling-v0\"))\n",
        "      state = env.reset()\n",
        "      writer.append_data(state)\n",
        "      state_ = self.get_frame(state)\n",
        "      stacked_frames = np.stack((state_,state_,state_,state_),axis=2)\n",
        "      stacked_frames = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2])\n",
        "      done=False\n",
        "      episode_reward=0  \n",
        "      while not done:\n",
        "        action=self.get_action(stacked_frames,\"Evaluating\")\n",
        "        next_state, reward, done, info=env.step(action)\n",
        "        writer.append_data(next_state) \n",
        "        next_state_ = self.get_frame(next_state)\n",
        "        next_state_ = next_state_.reshape(1,next_state_.shape[0],next_state_.shape[1],1)\n",
        "        stacked_frames = np.append(next_state_, stacked_frames[:, :, :, :3], axis=3)\n",
        "        episode_reward+=reward\n",
        "      Average_Reward.append(episode_reward)\n",
        "      print(\"Evaluating_Episode:{}  Reward:{} Average_Reward:{}\".format(episode,episode_reward,sum(Average_Reward)/len(Average_Reward)))\n",
        "      writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rv3Lg4TtTxW"
      },
      "source": [
        "Training the model\n",
        "\n",
        "This method creates a training environment for the model. Iterating through a set number of episodes, it uses the model to sample actions and play them. When such a timestep ends, the model is using the observations to update the policy.\n",
        "\n",
        "We know that in a dynamic game we cannot predict action based on 1 observation(which is 1 frame of the game in this case) so we will use a stack of 4 frames to predict the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1zvFiYLl-QN"
      },
      "source": [
        "  def train(self,no_of_episodes):\n",
        "    self.Average_rewards = []\n",
        "    for episode in range(no_of_episodes):\n",
        "      state = env.reset()\n",
        "      state_ = self.get_frame(state)\n",
        "      stacked_frames = np.stack((state_,state_,state_,state_),axis=2)\n",
        "      stacked_frames = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2])\n",
        "      done = False\n",
        "      episode_reward = 0\n",
        "      while not done:\n",
        "        action = self.get_action(stacked_frames)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = self.get_frame(next_state)\n",
        "        next_state_ = next_state.reshape(1,next_state.shape[0],next_state.shape[1],1)\n",
        "        next_state_ = np.append(next_state_, stacked_frames[:, :, :, :3], axis=3)\n",
        "\n",
        "        experience = stacked_frames, action, reward, 1*done\n",
        "        episode_reward+=reward\n",
        "        self.memory.store(experience)\n",
        "        stacked_frames = next_state_\n",
        "        \n",
        "      if episode%self.observing_episodes==0 and episode!=0:\n",
        "        self.update_training_model()\n",
        "         \n",
        "      if episode%self.observing_episodes_target_model==0 and episode!=0:\n",
        "        self.update_target_model()\n",
        "\n",
        "      self.Average_rewards.append(episode_reward)\n",
        "      avg_reward = np.mean(self.Average_rewards[-40:])\n",
        "\n",
        "      if self.epsilon_current_value > self.epsilon_final_value:\n",
        "        self.epsilon_current_value=self.epsilon_current_value-(self.epsilon_initial_value-self.epsilon_final_value)/1000.0\n",
        "      \n",
        "      print(\"Episode:{} Average Reward:{} Reward:{} Epsilon:{}\".format(episode,avg_reward,episode_reward,self.epsilon_current_value))\n",
        "\n",
        "\n",
        "      if episode%500==0 and episode!=0:\n",
        "\n",
        "        self.evaluate(episode,1)\n",
        "        \n",
        "        weights = self.training_model.get_weights()\n",
        "        with open(\"training_model_{}.txt\".format(episode), \"wb\") as fp:\n",
        "          pickle.dump(weights, fp)\n",
        "\n",
        "      #if self.temperature_parameter_current_value > self.temperature_parameter_final_value:\n",
        "        #self.temperature_parameter_current_value=self.temperature_parameter_current_value-(self.temperature_parameter_initial_value-self.temperature_parameter_final_value)/1000.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbEBOhadmhnk"
      },
      "source": [
        "no_of_episodes=1001\n",
        "\n",
        "Agent = Rainbow(env,memory_size)\n",
        "Agent.train(no_of_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zJgYAEgtjS0"
      },
      "source": [
        "With the help of below code we run our algorithm and see the success of it. With the help of below code we run our algorithm and see the success of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzFXfsG0uJyi"
      },
      "source": [
        "  def get_action(self, state,status='Training'):\n",
        "    '''samples the next action based on the E-greedy policy'''\n",
        "\n",
        "    if status==\"testing\":\n",
        "       _ , q_values = self.get_q_values(self.training_model(state))\n",
        "       action = np.argmax(q_values)\n",
        "       return action\n",
        "\n",
        "    if random.random() < self.epsilon_current_value:                                    #Exlporation\n",
        "      #_ , q_values = self.get_q_values(self.training_model.predict(state))\n",
        "      #q_values=(q_values[0])**(1/self.temperature_parameter_current_value)  #This is the step where we use Boltzmann exploration policy\n",
        "      #top_actions=q_values.argsort()[-self.nma:][::-1]\n",
        "      #action=random.choice(top_actions)\n",
        "      action = random.choice(list(range(self.action_shape)))\n",
        "    else:\n",
        "      _ , q_values = self.get_q_values(self.training_model(state)) #Exploitation\n",
        "      action = np.argmax(q_values)\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft23LLgzvhPS"
      },
      "source": [
        "With the help of below code we run our algorithm and see the success of it. With the help of below code we run our algorithm and see the success of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEctXzUQtjr0"
      },
      "source": [
        "class tester:\n",
        "  def __init__(self,path):\n",
        "    self.model = _rainbow_model(state_shape,action_shape,N_atoms)\n",
        "    with open(path, \"rb\") as fp:\n",
        "      weights = pickle.load(fp)\n",
        "    self.model(np.zeros(*state_shape));\n",
        "    self.model.set_weights(weights)\n",
        "      \n",
        "  def get_action(self, state):\n",
        "    '''samples the next action based on the E-greedy policy'''\n",
        "    _ , q_values = self.get_q_values(self.training_model(state))\n",
        "    action = np.argmax(q_values)\n",
        "    return action\n",
        "  \n",
        "  def get_frame(self,frame):\n",
        "    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame=frame[100:-34,:]\n",
        "    frame=frame/255.\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-MDKihLvgfP"
      },
      "source": [
        "writer = imageio.get_writer(\"test_video.mp4\", fps=20)\n",
        "env = (gym.make(\"Bowling-v0\"))\n",
        "state = env.reset()\n",
        "writer.append_data(state)\n",
        "state_ = test.get_frame(state)\n",
        "stacked_frames = np.stack((state_,state_,state_,state_),axis=2)\n",
        "stacked_frames = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2])\n",
        "episode_reward=0\n",
        "while True:\n",
        "  action = test.get_action(stacked_frames)\n",
        "  next_state, reward, done, info=env.step(action)\n",
        "  writer.append_data(next_state) \n",
        "  next_state_=test.get_frame(next_state)\n",
        "  next_state_ = next_state_.reshape(1,next_state_.shape[0],next_state_.shape[1],1)\n",
        "  stacked_frames = np.append(next_state_, stacked_frames[:, :, :, :3], axis=3)\n",
        "  episode_reward+=reward\n",
        "  if done:\n",
        "    break\n",
        "env.close()\n",
        "writer.close()\n",
        "print(\"Testing_Episode Reward:{}\".format(episode_reward)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}